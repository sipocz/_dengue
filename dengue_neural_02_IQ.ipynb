{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dengue_neural_02_IQ.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPeZE0ePKc1yarJMSsdK9xu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/_dengue/blob/main/dengue_neural_02_IQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0whmKbt9DSbQ"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#--------------scikit import \n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGsL2vrsC0rU",
        "outputId": "dc29b0b8-3c3b-436e-849f-4b7d68bd855c"
      },
      "source": [
        "#_Dengue\n",
        "_project=\"_dengue\"\n",
        "_PCVERSION_=False\n",
        "_GITHUBVERSION_=True\n",
        "_GOOGLEVERSION_=False\n",
        "if _PCVERSION_:\n",
        "    basedir=\"C:/Users/sipocz/OneDrive/Dokumentumok/GitHub\"\n",
        "\n",
        "if _GOOGLEVERSION_:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    basedir=\"/content/drive/My Drive/001_AI\"\n",
        "\n",
        "if _GITHUBVERSION_:\n",
        "    !mkdir _dengue\n",
        "    files=[\"dengue_features_train.csv\",\"dengue_labels_train.csv\",\"dengue_features_test.csv\",\"/models/'model___79_0.04485_0.02581_.hdf5'\"]\n",
        "    path=\"https://raw.githubusercontent.com/sipocz/_dengue/main/\"\n",
        "    basedir=\"./\"\n",
        "    storeto=basedir+_project\n",
        "    for f in files:\n",
        "        fname=path+f\n",
        "        !rm $f\n",
        "        !wget $fname\n",
        "        !mv $f $storeto"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘_dengue’: File exists\n",
            "rm: cannot remove 'dengue_features_train.csv': No such file or directory\n",
            "--2021-08-15 18:57:22--  https://raw.githubusercontent.com/sipocz/_dengue/main/dengue_features_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 287139 (280K) [text/plain]\n",
            "Saving to: ‘dengue_features_train.csv’\n",
            "\n",
            "dengue_features_tra 100%[===================>] 280.41K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-08-15 18:57:22 (18.0 MB/s) - ‘dengue_features_train.csv’ saved [287139/287139]\n",
            "\n",
            "rm: cannot remove 'dengue_labels_train.csv': No such file or directory\n",
            "--2021-08-15 18:57:22--  https://raw.githubusercontent.com/sipocz/_dengue/main/dengue_labels_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19582 (19K) [text/plain]\n",
            "Saving to: ‘dengue_labels_train.csv’\n",
            "\n",
            "dengue_labels_train 100%[===================>]  19.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-15 18:57:23 (75.0 MB/s) - ‘dengue_labels_train.csv’ saved [19582/19582]\n",
            "\n",
            "rm: cannot remove 'dengue_features_test.csv': No such file or directory\n",
            "--2021-08-15 18:57:23--  https://raw.githubusercontent.com/sipocz/_dengue/main/dengue_features_test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82465 (81K) [text/plain]\n",
            "Saving to: ‘dengue_features_test.csv’\n",
            "\n",
            "dengue_features_tes 100%[===================>]  80.53K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-08-15 18:57:23 (16.3 MB/s) - ‘dengue_features_test.csv’ saved [82465/82465]\n",
            "\n",
            "rm: cannot remove '/models/model___79_0.04485_0.02581_.hdf5': No such file or directory\n",
            "--2021-08-15 18:57:23--  https://raw.githubusercontent.com/sipocz/_dengue/main//models/model___79_0.04485_0.02581_.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sipocz/_dengue/main/models/model___79_0.04485_0.02581_.hdf5 [following]\n",
            "--2021-08-15 18:57:23--  https://raw.githubusercontent.com/sipocz/_dengue/main/models/model___79_0.04485_0.02581_.hdf5\n",
            "Reusing existing connection to raw.githubusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 386732 (378K) [application/octet-stream]\n",
            "Saving to: ‘model___79_0.04485_0.02581_.hdf5’\n",
            "\n",
            "model___79_0.04485_ 100%[===================>] 377.67K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-08-15 18:57:23 (23.1 MB/s) - ‘model___79_0.04485_0.02581_.hdf5’ saved [386732/386732]\n",
            "\n",
            "mv: cannot stat '/models/model___79_0.04485_0.02581_.hdf5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eptb9-bvDGXL"
      },
      "source": [
        "\n",
        "features_train=basedir+_project+\"/\"+\"dengue_features_train.csv\"\n",
        "labels_train=basedir+_project+\"/\"+\"dengue_labels_train.csv\"\n",
        "features_test=basedir+_project+\"/\"+\"dengue_features_test.csv\"\n",
        "\n",
        "\n",
        "X_train=pd.read_csv(features_train)\n",
        "y_train=pd.read_csv(labels_train)\n",
        "X_test=pd.read_csv(features_test)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRYy2LdXria_"
      },
      "source": [
        "X_train[\"total_cases\"]=y_train[\"total_cases\"]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24pyp2CPDZ8u"
      },
      "source": [
        "X_test.fillna(method=\"bfill\", inplace=True)\n",
        "X_train.fillna(method=\"bfill\", inplace=True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhcMI8ZeTx69"
      },
      "source": [
        "\n",
        "X_train_sj=X_train[X_train.city == \"sj\"]\n",
        "X_train_iq=X_train[X_train.city == \"iq\"]\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z2gUXspSzAi"
      },
      "source": [
        "\n",
        "X_test_sj=X_test[X_test.city == \"sj\"]\n",
        "X_test_iq=X_test[X_test.city == \"iq\"]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV9fDqjQNKzr"
      },
      "source": [
        "def build_training_data(dataset,labelset, history_size = 5, target_size = 23):\n",
        "    '''\n",
        "    source: https://www.mikulskibartosz.name/how-to-split-a-data-frame-into-time-series-for-lstm-deep-neural-network/\n",
        "    '''\n",
        "    start_index = history_size\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i - history_size, i, 1)\n",
        "        #print(indices)\n",
        "        row=[]\n",
        "        for j in indices:\n",
        "            row=row+list(dataset.iloc[j])\n",
        "        data.append(row)\n",
        "\n",
        "    #label \n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i, i+target_size, 1)\n",
        "        #print(indices)\n",
        "        row=[]\n",
        "        for j in indices:\n",
        "            row=row+list(labelset.iloc[j])\n",
        "        labels.append(row)\n",
        "\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels=np.array(labels)\n",
        "    #data = data.reshape((-1, 1))\n",
        "    return data, labels\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z1tYJIoDbe-"
      },
      "source": [
        "def datapipe_Xy(Xinp,yinp):\n",
        "    \n",
        "    X=Xinp.copy(deep=True)\n",
        "    y=yinp.copy(deep=True)\n",
        "    prediktorok=['ndvi_ne', 'ndvi_nw',\n",
        "       'ndvi_se', 'ndvi_sw',\n",
        "       'precipitation_amt_mm', 'reanalysis_air_temp_k',\n",
        "       'reanalysis_avg_temp_k', 'reanalysis_dew_point_temp_k',\n",
        "       'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k',\n",
        "       'reanalysis_precip_amt_kg_per_m2',\n",
        "       'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm',\n",
        "       'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k',\n",
        "       'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c',\n",
        "       'station_min_temp_c', 'station_precip_mm']\n",
        "    X[\"reanalysis_air_temp_k\"]=X[\"reanalysis_air_temp_k\"]-273.15\n",
        "    X[\"reanalysis_avg_temp_k\"]=X[\"reanalysis_avg_temp_k\"]-273.15\n",
        "    X['reanalysis_max_air_temp_k']=X['reanalysis_max_air_temp_k']-273.15\n",
        "    X[\"reanalysis_min_air_temp_k\"]=X[\"reanalysis_min_air_temp_k\"]-273.15\n",
        "    X[\"reanalysis_dew_point_temp_k\"]=X[\"reanalysis_dew_point_temp_k\"]-273.15\n",
        "    X[\"ndvi_N\"]=(X[\"ndvi_ne\"]+X[\"ndvi_nw\"])/2\n",
        "    X[\"ndvi_S\"]=(X[\"ndvi_se\"]+X[\"ndvi_sw\"])/2\n",
        "\n",
        "\n",
        "    Xprediktor=['ndvi_N','ndvi_S',\n",
        "       'precipitation_amt_mm', 'reanalysis_air_temp_k',\n",
        "       'reanalysis_avg_temp_k', 'reanalysis_dew_point_temp_k',\n",
        "       'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k',\n",
        "       'reanalysis_precip_amt_kg_per_m2',\n",
        "       'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm',\n",
        "       'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k',\n",
        "       'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c',\n",
        "       'station_min_temp_c', 'station_precip_mm']\n",
        "    \n",
        "    yprediktor=[\"total_cases\"]\n",
        "    y['total_cases']=y['total_cases']\n",
        "    \n",
        "    return(X[Xprediktor],y[yprediktor])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5wV38Y_Nwj6"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5weMYMnL0Ts"
      },
      "source": [
        "#test\n",
        "X2,y2=datapipe_Xy(X_train_iq,X_train_iq)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "o3NdSvd9Nxaq",
        "outputId": "1dc9fd44-bda0-4f0d-af77-2eb9bbefcf5f"
      },
      "source": [
        "y2.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_cases</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>936</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     total_cases\n",
              "936            0\n",
              "937            0\n",
              "938            0\n",
              "939            0\n",
              "940            0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "scTNFd-XLVBv",
        "outputId": "56ff8e42-36d3-4350-e965-3a5c733e85b3"
      },
      "source": [
        "X2.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ndvi_N</th>\n",
              "      <th>ndvi_S</th>\n",
              "      <th>precipitation_amt_mm</th>\n",
              "      <th>reanalysis_air_temp_k</th>\n",
              "      <th>reanalysis_avg_temp_k</th>\n",
              "      <th>reanalysis_dew_point_temp_k</th>\n",
              "      <th>reanalysis_max_air_temp_k</th>\n",
              "      <th>reanalysis_min_air_temp_k</th>\n",
              "      <th>reanalysis_precip_amt_kg_per_m2</th>\n",
              "      <th>reanalysis_relative_humidity_percent</th>\n",
              "      <th>reanalysis_sat_precip_amt_mm</th>\n",
              "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
              "      <th>reanalysis_tdtr_k</th>\n",
              "      <th>station_avg_temp_c</th>\n",
              "      <th>station_diur_temp_rng_c</th>\n",
              "      <th>station_max_temp_c</th>\n",
              "      <th>station_min_temp_c</th>\n",
              "      <th>station_precip_mm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>936</th>\n",
              "      <td>0.162571</td>\n",
              "      <td>0.294043</td>\n",
              "      <td>25.41</td>\n",
              "      <td>23.590000</td>\n",
              "      <td>25.300000</td>\n",
              "      <td>22.034286</td>\n",
              "      <td>34.15</td>\n",
              "      <td>19.95</td>\n",
              "      <td>43.19</td>\n",
              "      <td>92.418571</td>\n",
              "      <td>25.41</td>\n",
              "      <td>16.651429</td>\n",
              "      <td>8.928571</td>\n",
              "      <td>26.400000</td>\n",
              "      <td>10.775000</td>\n",
              "      <td>32.5</td>\n",
              "      <td>20.7</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>0.246467</td>\n",
              "      <td>0.265557</td>\n",
              "      <td>60.61</td>\n",
              "      <td>23.484286</td>\n",
              "      <td>25.278571</td>\n",
              "      <td>22.208571</td>\n",
              "      <td>33.45</td>\n",
              "      <td>17.95</td>\n",
              "      <td>46.00</td>\n",
              "      <td>93.581429</td>\n",
              "      <td>60.61</td>\n",
              "      <td>16.862857</td>\n",
              "      <td>10.314286</td>\n",
              "      <td>26.900000</td>\n",
              "      <td>11.566667</td>\n",
              "      <td>34.0</td>\n",
              "      <td>20.8</td>\n",
              "      <td>55.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>0.174943</td>\n",
              "      <td>0.166064</td>\n",
              "      <td>55.52</td>\n",
              "      <td>23.265714</td>\n",
              "      <td>24.242857</td>\n",
              "      <td>22.472857</td>\n",
              "      <td>31.35</td>\n",
              "      <td>19.45</td>\n",
              "      <td>64.77</td>\n",
              "      <td>95.848571</td>\n",
              "      <td>55.52</td>\n",
              "      <td>17.120000</td>\n",
              "      <td>7.385714</td>\n",
              "      <td>26.800000</td>\n",
              "      <td>11.466667</td>\n",
              "      <td>33.0</td>\n",
              "      <td>20.7</td>\n",
              "      <td>38.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>0.186579</td>\n",
              "      <td>0.227257</td>\n",
              "      <td>5.60</td>\n",
              "      <td>22.207143</td>\n",
              "      <td>23.078571</td>\n",
              "      <td>19.647143</td>\n",
              "      <td>30.45</td>\n",
              "      <td>15.45</td>\n",
              "      <td>23.96</td>\n",
              "      <td>87.234286</td>\n",
              "      <td>5.60</td>\n",
              "      <td>14.431429</td>\n",
              "      <td>9.114286</td>\n",
              "      <td>25.766667</td>\n",
              "      <td>10.533333</td>\n",
              "      <td>31.5</td>\n",
              "      <td>14.7</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>0.325386</td>\n",
              "      <td>0.307707</td>\n",
              "      <td>62.76</td>\n",
              "      <td>23.282857</td>\n",
              "      <td>24.485714</td>\n",
              "      <td>20.807143</td>\n",
              "      <td>33.85</td>\n",
              "      <td>18.35</td>\n",
              "      <td>31.80</td>\n",
              "      <td>88.161429</td>\n",
              "      <td>62.76</td>\n",
              "      <td>15.444286</td>\n",
              "      <td>9.500000</td>\n",
              "      <td>26.600000</td>\n",
              "      <td>11.480000</td>\n",
              "      <td>33.3</td>\n",
              "      <td>19.1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ndvi_N    ndvi_S  ...  station_min_temp_c  station_precip_mm\n",
              "936  0.162571  0.294043  ...                20.7                3.0\n",
              "937  0.246467  0.265557  ...                20.8               55.6\n",
              "938  0.174943  0.166064  ...                20.7               38.1\n",
              "939  0.186579  0.227257  ...                14.7               30.0\n",
              "940  0.325386  0.307707  ...                19.1                4.0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI4PcCj8kDf2"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATkEEguEb6W6"
      },
      "source": [
        "XIQ,yIQ=build_training_data(X2,y2,9,1)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FjcaOwjjYeh"
      },
      "source": [
        "#XSJ.reshape(-1,9,18)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlgmt1SvcCj2",
        "outputId": "0a4e4ffc-249e-47a8-da4f-c357c8f77ac2"
      },
      "source": [
        "len(XIQ[0])/9"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRDYJ9lQjOX5",
        "outputId": "17f77b98-ef36-4102-bd65-3d2149f4f3a2"
      },
      "source": [
        "print(yIQ[1])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD-sOBFYmbkG"
      },
      "source": [
        "from keras.backend import clear_session\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout,Flatten,Bidirectional\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta,Adagrad,Nadam,RMSprop\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "#import tensorflow_addons as tfa"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3ebXumcmtjE",
        "outputId": "30f89f2b-ff56-4c72-9258-2eea674b5ac5"
      },
      "source": [
        "clear_session() \n",
        "dropout_1_rate=0.15\n",
        "input_size=len(XIQ[0]) # timesteps\n",
        "output_size=len(yIQ[0]) # features\n",
        "learning_rate=0.00001\n",
        "\n",
        "inputs=Input(shape=(input_size))\n",
        "o1=Dense(500,activation=\"relu\",kernel_initializer=\"HeNormal\")(inputs)\n",
        "d1=Dropout(dropout_1_rate,)(o1)\n",
        "o2=Dense(2000,activation=\"relu\",kernel_initializer=\"HeNormal\")(d1)\n",
        "d2=Dropout(dropout_1_rate)(o2)\n",
        "\n",
        "o3=Dense(1000,activation=\"relu\",kernel_initializer=\"HeNormal\")(d2)\n",
        "#d3=Dropout(dropout_1_rate)(o3)\n",
        "\n",
        "result=Dense(output_size,activation=\"tanh\",)(o3)\n",
        "#dropout1=Dropout(rate=dropout_1_rate)(layer2)\n",
        "#result=Dense(output_size,activation=\"sigmoid\")(dropout1)\n",
        "\n",
        "model_IQ = Model(inputs=inputs, outputs=result)\n",
        "model_IQ.compile(optimizer=Adamax(learning_rate=learning_rate,), loss=\"mae\")  #RMSprop\n",
        "model_IQ.summary()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 162)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               81500     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2000)              1002000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              2001000   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 1001      \n",
            "=================================================================\n",
            "Total params: 3,085,501\n",
            "Trainable params: 3,085,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmoQ0-8Incy8"
      },
      "source": [
        ""
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6h8N5NrpbwH"
      },
      "source": [
        "\n",
        "x_scaler = MinMaxScaler()\n",
        "XIQ_scale = pd.DataFrame(x_scaler.fit_transform(XIQ))\n",
        "\n",
        "y_scaler=MinMaxScaler(feature_range=(-1,1))\n",
        "yIQ_scale=pd.DataFrame(y_scaler.fit_transform(yIQ))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "EZ6f8mr5w5uV",
        "outputId": "f08128cf-17bf-43e6-9e53-f5cf5742746a"
      },
      "source": [
        "yIQ_scale"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.982759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>-0.896552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>-0.913793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>-0.862069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>-0.982759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>-0.982759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>510 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0\n",
              "0   -1.000000\n",
              "1   -0.982759\n",
              "2   -1.000000\n",
              "3   -1.000000\n",
              "4   -1.000000\n",
              "..        ...\n",
              "505 -0.896552\n",
              "506 -0.913793\n",
              "507 -0.862069\n",
              "508 -0.982759\n",
              "509 -0.982759\n",
              "\n",
              "[510 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXBjot_lP3Mv"
      },
      "source": [
        "!rm model*"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q5ghx0VORBA"
      },
      "source": [
        "fname=\"./model_IQ\"\n",
        "callbacks = [#callback_LR,\n",
        "       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch}\"+\"_{loss:.5f}_{val_loss:.5f}_.hdf5\", monitor='val_loss',\n",
        "                        verbose=2, save_best_only=True, mode='min',)]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "WYy19t3r0255",
        "outputId": "89c039ea-ea27-4a8f-d7d0-45ea7934e7e0"
      },
      "source": [
        "XIQ_scale"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.228962</td>\n",
              "      <td>0.471363</td>\n",
              "      <td>0.120524</td>\n",
              "      <td>0.300551</td>\n",
              "      <td>0.442667</td>\n",
              "      <td>0.609431</td>\n",
              "      <td>0.521429</td>\n",
              "      <td>0.681319</td>\n",
              "      <td>0.119300</td>\n",
              "      <td>0.848334</td>\n",
              "      <td>0.120524</td>\n",
              "      <td>0.543713</td>\n",
              "      <td>0.423434</td>\n",
              "      <td>0.531915</td>\n",
              "      <td>0.525943</td>\n",
              "      <td>0.198347</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.005522</td>\n",
              "      <td>0.431854</td>\n",
              "      <td>0.406603</td>\n",
              "      <td>0.287483</td>\n",
              "      <td>0.285452</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.630275</td>\n",
              "      <td>0.471429</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.127061</td>\n",
              "      <td>0.876820</td>\n",
              "      <td>0.287483</td>\n",
              "      <td>0.569033</td>\n",
              "      <td>0.535963</td>\n",
              "      <td>0.585106</td>\n",
              "      <td>0.600629</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.102338</td>\n",
              "      <td>0.258881</td>\n",
              "      <td>0.180413</td>\n",
              "      <td>0.263340</td>\n",
              "      <td>0.254234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.608491</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.448718</td>\n",
              "      <td>0.134180</td>\n",
              "      <td>0.774590</td>\n",
              "      <td>0.626461</td>\n",
              "      <td>0.199592</td>\n",
              "      <td>0.427260</td>\n",
              "      <td>0.552889</td>\n",
              "      <td>0.466940</td>\n",
              "      <td>0.564286</td>\n",
              "      <td>0.406593</td>\n",
              "      <td>0.046681</td>\n",
              "      <td>0.605123</td>\n",
              "      <td>0.199592</td>\n",
              "      <td>0.401711</td>\n",
              "      <td>0.810905</td>\n",
              "      <td>0.551418</td>\n",
              "      <td>0.482704</td>\n",
              "      <td>0.239669</td>\n",
              "      <td>0.525641</td>\n",
              "      <td>0.092214</td>\n",
              "      <td>0.718895</td>\n",
              "      <td>0.609295</td>\n",
              "      <td>0.233458</td>\n",
              "      <td>0.514589</td>\n",
              "      <td>0.582222</td>\n",
              "      <td>0.336750</td>\n",
              "      <td>0.721429</td>\n",
              "      <td>0.483516</td>\n",
              "      <td>0.015441</td>\n",
              "      <td>0.415279</td>\n",
              "      <td>0.233458</td>\n",
              "      <td>0.279384</td>\n",
              "      <td>0.816705</td>\n",
              "      <td>0.585106</td>\n",
              "      <td>0.773585</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.164182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.431854</td>\n",
              "      <td>0.406603</td>\n",
              "      <td>0.287483</td>\n",
              "      <td>0.285452</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.630275</td>\n",
              "      <td>0.471429</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.127061</td>\n",
              "      <td>0.876820</td>\n",
              "      <td>0.287483</td>\n",
              "      <td>0.569033</td>\n",
              "      <td>0.535963</td>\n",
              "      <td>0.585106</td>\n",
              "      <td>0.600629</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.642105</td>\n",
              "      <td>0.102338</td>\n",
              "      <td>0.258881</td>\n",
              "      <td>0.180413</td>\n",
              "      <td>0.263340</td>\n",
              "      <td>0.254234</td>\n",
              "      <td>0.311111</td>\n",
              "      <td>0.661883</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.626374</td>\n",
              "      <td>0.178908</td>\n",
              "      <td>0.932356</td>\n",
              "      <td>0.263340</td>\n",
              "      <td>0.599829</td>\n",
              "      <td>0.298144</td>\n",
              "      <td>0.574468</td>\n",
              "      <td>0.591195</td>\n",
              "      <td>0.239669</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.070127</td>\n",
              "      <td>0.287021</td>\n",
              "      <td>0.319530</td>\n",
              "      <td>0.026562</td>\n",
              "      <td>0.103040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.482704</td>\n",
              "      <td>0.239669</td>\n",
              "      <td>0.525641</td>\n",
              "      <td>0.092214</td>\n",
              "      <td>0.718895</td>\n",
              "      <td>0.609295</td>\n",
              "      <td>0.233458</td>\n",
              "      <td>0.514589</td>\n",
              "      <td>0.582222</td>\n",
              "      <td>0.336750</td>\n",
              "      <td>0.721429</td>\n",
              "      <td>0.483516</td>\n",
              "      <td>0.015441</td>\n",
              "      <td>0.415279</td>\n",
              "      <td>0.233458</td>\n",
              "      <td>0.279384</td>\n",
              "      <td>0.816705</td>\n",
              "      <td>0.585106</td>\n",
              "      <td>0.773585</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.164182</td>\n",
              "      <td>0.625528</td>\n",
              "      <td>0.529092</td>\n",
              "      <td>0.254470</td>\n",
              "      <td>0.654560</td>\n",
              "      <td>0.751111</td>\n",
              "      <td>0.404750</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.659341</td>\n",
              "      <td>0.044389</td>\n",
              "      <td>0.400861</td>\n",
              "      <td>0.254470</td>\n",
              "      <td>0.352780</td>\n",
              "      <td>0.709977</td>\n",
              "      <td>0.608156</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.143567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.258881</td>\n",
              "      <td>0.180413</td>\n",
              "      <td>0.263340</td>\n",
              "      <td>0.254234</td>\n",
              "      <td>0.311111</td>\n",
              "      <td>0.661883</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.626374</td>\n",
              "      <td>0.178908</td>\n",
              "      <td>0.932356</td>\n",
              "      <td>0.263340</td>\n",
              "      <td>0.599829</td>\n",
              "      <td>0.298144</td>\n",
              "      <td>0.574468</td>\n",
              "      <td>0.591195</td>\n",
              "      <td>0.239669</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.070127</td>\n",
              "      <td>0.287021</td>\n",
              "      <td>0.319530</td>\n",
              "      <td>0.026562</td>\n",
              "      <td>0.103040</td>\n",
              "      <td>0.166222</td>\n",
              "      <td>0.323936</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>0.186813</td>\n",
              "      <td>0.066182</td>\n",
              "      <td>0.721340</td>\n",
              "      <td>0.026562</td>\n",
              "      <td>0.277844</td>\n",
              "      <td>0.438515</td>\n",
              "      <td>0.464539</td>\n",
              "      <td>0.503145</td>\n",
              "      <td>0.115702</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055218</td>\n",
              "      <td>0.622712</td>\n",
              "      <td>0.502428</td>\n",
              "      <td>0.297681</td>\n",
              "      <td>0.256682</td>\n",
              "      <td>...</td>\n",
              "      <td>0.773585</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.164182</td>\n",
              "      <td>0.625528</td>\n",
              "      <td>0.529092</td>\n",
              "      <td>0.254470</td>\n",
              "      <td>0.654560</td>\n",
              "      <td>0.751111</td>\n",
              "      <td>0.404750</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.659341</td>\n",
              "      <td>0.044389</td>\n",
              "      <td>0.400861</td>\n",
              "      <td>0.254470</td>\n",
              "      <td>0.352780</td>\n",
              "      <td>0.709977</td>\n",
              "      <td>0.608156</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.143567</td>\n",
              "      <td>0.550762</td>\n",
              "      <td>0.460061</td>\n",
              "      <td>0.109662</td>\n",
              "      <td>0.880841</td>\n",
              "      <td>0.876444</td>\n",
              "      <td>0.065437</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.505495</td>\n",
              "      <td>0.006906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109662</td>\n",
              "      <td>0.064842</td>\n",
              "      <td>0.908353</td>\n",
              "      <td>0.741135</td>\n",
              "      <td>0.726415</td>\n",
              "      <td>0.471074</td>\n",
              "      <td>0.679487</td>\n",
              "      <td>0.104730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.287021</td>\n",
              "      <td>0.319530</td>\n",
              "      <td>0.026562</td>\n",
              "      <td>0.103040</td>\n",
              "      <td>0.166222</td>\n",
              "      <td>0.323936</td>\n",
              "      <td>0.257143</td>\n",
              "      <td>0.186813</td>\n",
              "      <td>0.066182</td>\n",
              "      <td>0.721340</td>\n",
              "      <td>0.026562</td>\n",
              "      <td>0.277844</td>\n",
              "      <td>0.438515</td>\n",
              "      <td>0.464539</td>\n",
              "      <td>0.503145</td>\n",
              "      <td>0.115702</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055218</td>\n",
              "      <td>0.622712</td>\n",
              "      <td>0.502428</td>\n",
              "      <td>0.297681</td>\n",
              "      <td>0.256682</td>\n",
              "      <td>0.341333</td>\n",
              "      <td>0.462669</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.505495</td>\n",
              "      <td>0.087838</td>\n",
              "      <td>0.744051</td>\n",
              "      <td>0.297681</td>\n",
              "      <td>0.399145</td>\n",
              "      <td>0.469838</td>\n",
              "      <td>0.553191</td>\n",
              "      <td>0.592453</td>\n",
              "      <td>0.264463</td>\n",
              "      <td>0.463158</td>\n",
              "      <td>0.007362</td>\n",
              "      <td>0.314988</td>\n",
              "      <td>0.356441</td>\n",
              "      <td>0.077029</td>\n",
              "      <td>0.365028</td>\n",
              "      <td>...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.143567</td>\n",
              "      <td>0.550762</td>\n",
              "      <td>0.460061</td>\n",
              "      <td>0.109662</td>\n",
              "      <td>0.880841</td>\n",
              "      <td>0.876444</td>\n",
              "      <td>0.065437</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.505495</td>\n",
              "      <td>0.006906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109662</td>\n",
              "      <td>0.064842</td>\n",
              "      <td>0.908353</td>\n",
              "      <td>0.741135</td>\n",
              "      <td>0.726415</td>\n",
              "      <td>0.471074</td>\n",
              "      <td>0.679487</td>\n",
              "      <td>0.104730</td>\n",
              "      <td>0.554565</td>\n",
              "      <td>0.616811</td>\n",
              "      <td>0.164208</td>\n",
              "      <td>0.745970</td>\n",
              "      <td>0.806222</td>\n",
              "      <td>0.424739</td>\n",
              "      <td>0.764286</td>\n",
              "      <td>0.593407</td>\n",
              "      <td>0.011325</td>\n",
              "      <td>0.339831</td>\n",
              "      <td>0.164208</td>\n",
              "      <td>0.373139</td>\n",
              "      <td>0.823666</td>\n",
              "      <td>0.640957</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.525641</td>\n",
              "      <td>0.034787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.622712</td>\n",
              "      <td>0.502428</td>\n",
              "      <td>0.297681</td>\n",
              "      <td>0.256682</td>\n",
              "      <td>0.341333</td>\n",
              "      <td>0.462669</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.505495</td>\n",
              "      <td>0.087838</td>\n",
              "      <td>0.744051</td>\n",
              "      <td>0.297681</td>\n",
              "      <td>0.399145</td>\n",
              "      <td>0.469838</td>\n",
              "      <td>0.553191</td>\n",
              "      <td>0.592453</td>\n",
              "      <td>0.264463</td>\n",
              "      <td>0.463158</td>\n",
              "      <td>0.007362</td>\n",
              "      <td>0.314988</td>\n",
              "      <td>0.356441</td>\n",
              "      <td>0.077029</td>\n",
              "      <td>0.365028</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.195797</td>\n",
              "      <td>0.442857</td>\n",
              "      <td>0.175824</td>\n",
              "      <td>0.002762</td>\n",
              "      <td>0.414999</td>\n",
              "      <td>0.077029</td>\n",
              "      <td>0.156886</td>\n",
              "      <td>0.816705</td>\n",
              "      <td>0.419149</td>\n",
              "      <td>0.541509</td>\n",
              "      <td>0.157025</td>\n",
              "      <td>0.242105</td>\n",
              "      <td>0.021167</td>\n",
              "      <td>0.612676</td>\n",
              "      <td>0.675712</td>\n",
              "      <td>0.423896</td>\n",
              "      <td>0.383391</td>\n",
              "      <td>...</td>\n",
              "      <td>0.726415</td>\n",
              "      <td>0.471074</td>\n",
              "      <td>0.679487</td>\n",
              "      <td>0.104730</td>\n",
              "      <td>0.554565</td>\n",
              "      <td>0.616811</td>\n",
              "      <td>0.164208</td>\n",
              "      <td>0.745970</td>\n",
              "      <td>0.806222</td>\n",
              "      <td>0.424739</td>\n",
              "      <td>0.764286</td>\n",
              "      <td>0.593407</td>\n",
              "      <td>0.011325</td>\n",
              "      <td>0.339831</td>\n",
              "      <td>0.164208</td>\n",
              "      <td>0.373139</td>\n",
              "      <td>0.823666</td>\n",
              "      <td>0.640957</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.525641</td>\n",
              "      <td>0.034787</td>\n",
              "      <td>0.615166</td>\n",
              "      <td>0.538803</td>\n",
              "      <td>0.462695</td>\n",
              "      <td>0.399918</td>\n",
              "      <td>0.450667</td>\n",
              "      <td>0.313173</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.008839</td>\n",
              "      <td>0.482573</td>\n",
              "      <td>0.462695</td>\n",
              "      <td>0.266724</td>\n",
              "      <td>0.617169</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.694969</td>\n",
              "      <td>0.487603</td>\n",
              "      <td>0.525641</td>\n",
              "      <td>0.191791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.495504</td>\n",
              "      <td>0.511180</td>\n",
              "      <td>0.574871</td>\n",
              "      <td>0.650684</td>\n",
              "      <td>0.751111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.528571</td>\n",
              "      <td>0.780220</td>\n",
              "      <td>0.529680</td>\n",
              "      <td>0.939110</td>\n",
              "      <td>0.574871</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.415313</td>\n",
              "      <td>0.914894</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>0.487603</td>\n",
              "      <td>0.768421</td>\n",
              "      <td>0.042150</td>\n",
              "      <td>0.522043</td>\n",
              "      <td>0.475520</td>\n",
              "      <td>0.243277</td>\n",
              "      <td>0.623954</td>\n",
              "      <td>0.672889</td>\n",
              "      <td>0.893046</td>\n",
              "      <td>0.564286</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.593597</td>\n",
              "      <td>0.864607</td>\n",
              "      <td>0.243277</td>\n",
              "      <td>0.869119</td>\n",
              "      <td>0.350348</td>\n",
              "      <td>0.645390</td>\n",
              "      <td>0.389937</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.663158</td>\n",
              "      <td>0.036812</td>\n",
              "      <td>0.254378</td>\n",
              "      <td>0.253260</td>\n",
              "      <td>0.236209</td>\n",
              "      <td>0.623750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.653302</td>\n",
              "      <td>0.504132</td>\n",
              "      <td>0.641026</td>\n",
              "      <td>0.065157</td>\n",
              "      <td>0.460472</td>\n",
              "      <td>0.482698</td>\n",
              "      <td>0.350851</td>\n",
              "      <td>0.425831</td>\n",
              "      <td>0.459556</td>\n",
              "      <td>0.823680</td>\n",
              "      <td>0.335714</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.236058</td>\n",
              "      <td>0.953527</td>\n",
              "      <td>0.350851</td>\n",
              "      <td>0.777417</td>\n",
              "      <td>0.162413</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.256198</td>\n",
              "      <td>0.641026</td>\n",
              "      <td>0.014909</td>\n",
              "      <td>0.231432</td>\n",
              "      <td>0.119647</td>\n",
              "      <td>0.281744</td>\n",
              "      <td>0.377474</td>\n",
              "      <td>0.378667</td>\n",
              "      <td>0.795319</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.780220</td>\n",
              "      <td>0.241140</td>\n",
              "      <td>0.971480</td>\n",
              "      <td>0.281744</td>\n",
              "      <td>0.752096</td>\n",
              "      <td>0.200696</td>\n",
              "      <td>0.638298</td>\n",
              "      <td>0.490566</td>\n",
              "      <td>0.297521</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.058899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>0.522043</td>\n",
              "      <td>0.475520</td>\n",
              "      <td>0.243277</td>\n",
              "      <td>0.623954</td>\n",
              "      <td>0.672889</td>\n",
              "      <td>0.893046</td>\n",
              "      <td>0.564286</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.593597</td>\n",
              "      <td>0.864607</td>\n",
              "      <td>0.243277</td>\n",
              "      <td>0.869119</td>\n",
              "      <td>0.350348</td>\n",
              "      <td>0.645390</td>\n",
              "      <td>0.389937</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.663158</td>\n",
              "      <td>0.036812</td>\n",
              "      <td>0.254378</td>\n",
              "      <td>0.253260</td>\n",
              "      <td>0.236209</td>\n",
              "      <td>0.623750</td>\n",
              "      <td>0.724444</td>\n",
              "      <td>0.774987</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.066983</td>\n",
              "      <td>0.757419</td>\n",
              "      <td>0.236209</td>\n",
              "      <td>0.730881</td>\n",
              "      <td>0.470998</td>\n",
              "      <td>0.695035</td>\n",
              "      <td>0.669811</td>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.663158</td>\n",
              "      <td>0.029450</td>\n",
              "      <td>0.459263</td>\n",
              "      <td>0.328478</td>\n",
              "      <td>0.444718</td>\n",
              "      <td>0.639869</td>\n",
              "      <td>...</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.256198</td>\n",
              "      <td>0.641026</td>\n",
              "      <td>0.014909</td>\n",
              "      <td>0.231432</td>\n",
              "      <td>0.119647</td>\n",
              "      <td>0.281744</td>\n",
              "      <td>0.377474</td>\n",
              "      <td>0.378667</td>\n",
              "      <td>0.795319</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.780220</td>\n",
              "      <td>0.241140</td>\n",
              "      <td>0.971480</td>\n",
              "      <td>0.281744</td>\n",
              "      <td>0.752096</td>\n",
              "      <td>0.200696</td>\n",
              "      <td>0.638298</td>\n",
              "      <td>0.490566</td>\n",
              "      <td>0.297521</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.058899</td>\n",
              "      <td>0.483412</td>\n",
              "      <td>0.374385</td>\n",
              "      <td>0.005455</td>\n",
              "      <td>0.430320</td>\n",
              "      <td>0.474667</td>\n",
              "      <td>0.375363</td>\n",
              "      <td>0.621429</td>\n",
              "      <td>0.351648</td>\n",
              "      <td>0.024307</td>\n",
              "      <td>0.519597</td>\n",
              "      <td>0.005455</td>\n",
              "      <td>0.334987</td>\n",
              "      <td>0.611369</td>\n",
              "      <td>0.450355</td>\n",
              "      <td>0.377358</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.004602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>0.254378</td>\n",
              "      <td>0.253260</td>\n",
              "      <td>0.236209</td>\n",
              "      <td>0.623750</td>\n",
              "      <td>0.724444</td>\n",
              "      <td>0.774987</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.066983</td>\n",
              "      <td>0.757419</td>\n",
              "      <td>0.236209</td>\n",
              "      <td>0.730881</td>\n",
              "      <td>0.470998</td>\n",
              "      <td>0.695035</td>\n",
              "      <td>0.669811</td>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.663158</td>\n",
              "      <td>0.029450</td>\n",
              "      <td>0.459263</td>\n",
              "      <td>0.328478</td>\n",
              "      <td>0.444718</td>\n",
              "      <td>0.639869</td>\n",
              "      <td>0.669333</td>\n",
              "      <td>0.965488</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.454935</td>\n",
              "      <td>0.916048</td>\n",
              "      <td>0.444718</td>\n",
              "      <td>0.954149</td>\n",
              "      <td>0.324826</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.669811</td>\n",
              "      <td>0.438017</td>\n",
              "      <td>0.821053</td>\n",
              "      <td>0.033131</td>\n",
              "      <td>0.215850</td>\n",
              "      <td>0.166512</td>\n",
              "      <td>0.082199</td>\n",
              "      <td>0.524179</td>\n",
              "      <td>...</td>\n",
              "      <td>0.490566</td>\n",
              "      <td>0.297521</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.058899</td>\n",
              "      <td>0.483412</td>\n",
              "      <td>0.374385</td>\n",
              "      <td>0.005455</td>\n",
              "      <td>0.430320</td>\n",
              "      <td>0.474667</td>\n",
              "      <td>0.375363</td>\n",
              "      <td>0.621429</td>\n",
              "      <td>0.351648</td>\n",
              "      <td>0.024307</td>\n",
              "      <td>0.519597</td>\n",
              "      <td>0.005455</td>\n",
              "      <td>0.334987</td>\n",
              "      <td>0.611369</td>\n",
              "      <td>0.450355</td>\n",
              "      <td>0.377358</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.004602</td>\n",
              "      <td>0.635866</td>\n",
              "      <td>0.426771</td>\n",
              "      <td>0.262297</td>\n",
              "      <td>0.671088</td>\n",
              "      <td>0.731556</td>\n",
              "      <td>0.805741</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.124299</td>\n",
              "      <td>0.758854</td>\n",
              "      <td>0.262297</td>\n",
              "      <td>0.763388</td>\n",
              "      <td>0.494200</td>\n",
              "      <td>0.769504</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>0.438017</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.049696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>0.459263</td>\n",
              "      <td>0.328478</td>\n",
              "      <td>0.444718</td>\n",
              "      <td>0.639869</td>\n",
              "      <td>0.669333</td>\n",
              "      <td>0.965488</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.454935</td>\n",
              "      <td>0.916048</td>\n",
              "      <td>0.444718</td>\n",
              "      <td>0.954149</td>\n",
              "      <td>0.324826</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.669811</td>\n",
              "      <td>0.438017</td>\n",
              "      <td>0.821053</td>\n",
              "      <td>0.033131</td>\n",
              "      <td>0.215850</td>\n",
              "      <td>0.166512</td>\n",
              "      <td>0.082199</td>\n",
              "      <td>0.524179</td>\n",
              "      <td>0.580444</td>\n",
              "      <td>0.826926</td>\n",
              "      <td>0.521429</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.416540</td>\n",
              "      <td>0.878639</td>\n",
              "      <td>0.082199</td>\n",
              "      <td>0.786313</td>\n",
              "      <td>0.285383</td>\n",
              "      <td>0.611702</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.239669</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.033131</td>\n",
              "      <td>0.472046</td>\n",
              "      <td>0.457690</td>\n",
              "      <td>0.411232</td>\n",
              "      <td>0.543154</td>\n",
              "      <td>...</td>\n",
              "      <td>0.377358</td>\n",
              "      <td>0.322314</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.004602</td>\n",
              "      <td>0.635866</td>\n",
              "      <td>0.426771</td>\n",
              "      <td>0.262297</td>\n",
              "      <td>0.671088</td>\n",
              "      <td>0.731556</td>\n",
              "      <td>0.805741</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.124299</td>\n",
              "      <td>0.758854</td>\n",
              "      <td>0.262297</td>\n",
              "      <td>0.763388</td>\n",
              "      <td>0.494200</td>\n",
              "      <td>0.769504</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>0.438017</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.049696</td>\n",
              "      <td>0.223382</td>\n",
              "      <td>0.214027</td>\n",
              "      <td>0.410141</td>\n",
              "      <td>0.527647</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.761148</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.549451</td>\n",
              "      <td>0.572052</td>\n",
              "      <td>0.828282</td>\n",
              "      <td>0.410141</td>\n",
              "      <td>0.713601</td>\n",
              "      <td>0.305104</td>\n",
              "      <td>0.641844</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.380165</td>\n",
              "      <td>0.679487</td>\n",
              "      <td>0.067366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>0.215850</td>\n",
              "      <td>0.166512</td>\n",
              "      <td>0.082199</td>\n",
              "      <td>0.524179</td>\n",
              "      <td>0.580444</td>\n",
              "      <td>0.826926</td>\n",
              "      <td>0.521429</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.416540</td>\n",
              "      <td>0.878639</td>\n",
              "      <td>0.082199</td>\n",
              "      <td>0.786313</td>\n",
              "      <td>0.285383</td>\n",
              "      <td>0.611702</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.239669</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.033131</td>\n",
              "      <td>0.472046</td>\n",
              "      <td>0.457690</td>\n",
              "      <td>0.411232</td>\n",
              "      <td>0.543154</td>\n",
              "      <td>0.574222</td>\n",
              "      <td>0.907740</td>\n",
              "      <td>0.335714</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.224843</td>\n",
              "      <td>0.935960</td>\n",
              "      <td>0.411232</td>\n",
              "      <td>0.878700</td>\n",
              "      <td>0.328306</td>\n",
              "      <td>0.686170</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.821053</td>\n",
              "      <td>0.094055</td>\n",
              "      <td>0.439207</td>\n",
              "      <td>0.502785</td>\n",
              "      <td>0.123322</td>\n",
              "      <td>0.630280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>0.438017</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.049696</td>\n",
              "      <td>0.223382</td>\n",
              "      <td>0.214027</td>\n",
              "      <td>0.410141</td>\n",
              "      <td>0.527647</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.761148</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.549451</td>\n",
              "      <td>0.572052</td>\n",
              "      <td>0.828282</td>\n",
              "      <td>0.410141</td>\n",
              "      <td>0.713601</td>\n",
              "      <td>0.305104</td>\n",
              "      <td>0.641844</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.380165</td>\n",
              "      <td>0.679487</td>\n",
              "      <td>0.067366</td>\n",
              "      <td>0.311153</td>\n",
              "      <td>0.353128</td>\n",
              "      <td>0.279562</td>\n",
              "      <td>0.280351</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.647360</td>\n",
              "      <td>0.392857</td>\n",
              "      <td>0.604396</td>\n",
              "      <td>0.139767</td>\n",
              "      <td>0.893932</td>\n",
              "      <td>0.279562</td>\n",
              "      <td>0.586484</td>\n",
              "      <td>0.307425</td>\n",
              "      <td>0.319149</td>\n",
              "      <td>0.160377</td>\n",
              "      <td>0.173554</td>\n",
              "      <td>0.358974</td>\n",
              "      <td>0.013620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>510 rows × 162 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2    ...       159       160       161\n",
              "0    0.228962  0.471363  0.120524  ...  0.322314  0.333333  0.164182\n",
              "1    0.431854  0.406603  0.287483  ...  0.322314  0.461538  0.143567\n",
              "2    0.258881  0.180413  0.263340  ...  0.471074  0.679487  0.104730\n",
              "3    0.287021  0.319530  0.026562  ...  0.363636  0.525641  0.034787\n",
              "4    0.622712  0.502428  0.297681  ...  0.487603  0.525641  0.191791\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "505  0.495504  0.511180  0.574871  ...  0.297521  0.615385  0.058899\n",
              "506  0.522043  0.475520  0.243277  ...  0.322314  0.461538  0.004602\n",
              "507  0.254378  0.253260  0.236209  ...  0.438017  0.769231  0.049696\n",
              "508  0.459263  0.328478  0.444718  ...  0.380165  0.679487  0.067366\n",
              "509  0.215850  0.166512  0.082199  ...  0.173554  0.358974  0.013620\n",
              "\n",
              "[510 rows x 162 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31wdXg6jokz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda82772-9d1e-481d-e548-325be3cc523d"
      },
      "source": [
        "\n",
        "from keras import backend \n",
        "backend.set_value(model_IQ.optimizer.learning_rate, 0.00001) # 0.0001--\n",
        "\n",
        "model_IQ.optimizer.iterations"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'iter:0' shape=() dtype=int64, numpy=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9zR35uKHvNu",
        "outputId": "b699ea75-f137-4cfd-876b-1a342e7aceff"
      },
      "source": [
        "history = model_IQ.fit(\n",
        "    x=XIQ_scale,\n",
        "    y=yIQ_scale,\n",
        "    validation_split=0.1,\n",
        "    epochs=15,\n",
        "    batch_size=17,\n",
        "    callbacks=callbacks,shuffle=True\n",
        "   )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "27/27 [==============================] - 2s 38ms/step - loss: 0.4677 - val_loss: 0.0800\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.08004, saving model to ./model_IQ_1_0.28026_0.08004_.hdf5\n",
            "Epoch 2/15\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.1346 - val_loss: 0.0813\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.08004\n",
            "Epoch 3/15\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 0.1279 - val_loss: 0.0820\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.08004\n",
            "Epoch 4/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1308 - val_loss: 0.0821\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.08004\n",
            "Epoch 5/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1341 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.08004\n",
            "Epoch 6/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1334 - val_loss: 0.0831\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.08004\n",
            "Epoch 7/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1205 - val_loss: 0.0830\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.08004\n",
            "Epoch 8/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1265 - val_loss: 0.0828\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.08004\n",
            "Epoch 9/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1210 - val_loss: 0.0829\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.08004\n",
            "Epoch 10/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1381 - val_loss: 0.0826\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.08004\n",
            "Epoch 11/15\n",
            "27/27 [==============================] - 1s 23ms/step - loss: 0.1123 - val_loss: 0.0822\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.08004\n",
            "Epoch 12/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1260 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.08004\n",
            "Epoch 13/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1256 - val_loss: 0.0818\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.08004\n",
            "Epoch 14/15\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.1187 - val_loss: 0.0817\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.08004\n",
            "Epoch 15/15\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 0.1242 - val_loss: 0.0815\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.08004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL5Wr_NkdsN0"
      },
      "source": [
        "def grafikon(fx,ind,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,0,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    ind: index\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\"--\"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(rows=1, cols=1)\n",
        "    \n",
        "    if True:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx[ind], y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "\n",
        "            row=1, col=1\n",
        "\n",
        "        )\n",
        "    \n",
        "    if ngraf>1:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx[ind], y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "\n",
        "            row=1, col=1\n",
        "        )\n",
        "    if ngraf>2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx[ind], y=fx[desc3], name=txt3, line=dict(color=c3) ,showlegend=True  ),\n",
        "\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    fig0.show()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FAjyiKgpoij"
      },
      "source": [
        "#model_loaded=tf.keras.models.load_model('model_IQ_115_0.11411_0.05609_.hdf5',)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QCA3rvVGLXL"
      },
      "source": [
        "model_loaded=model_IQ"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdlFfbUiWvra"
      },
      "source": [
        "y_pred=model_loaded.predict(XIQ_scale)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdZLGOUfXGpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae9190c-ddc2-49b7-a40d-a77560a69b5a"
      },
      "source": [
        "y_pred[1]\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.9535106], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck4uVgpwJzLH"
      },
      "source": [
        "def backtest_train():\n",
        "    y_orig=yIQ_scale\n",
        "    output=pd.DataFrame(y_pred)\n",
        "    output[\"a\"]=y_orig\n",
        "    output.columns=[\"pred\",\"orig\"]\n",
        "    output[\"index\"]=range(0,len(y_orig))\n",
        "    grafikon(output,\"index\",\"orig\",\"orig\",\"pred\",\"pred\")\n",
        "\n",
        "def backtest_test():\n",
        "    y_orig=yIQ_scale\n",
        "    output=pd.DataFrame(y_pred)\n",
        "    output[\"a\"]=y_orig\n",
        "    output.columns=[\"pred\",\"orig\"]\n",
        "    output[\"index\"]=range(0,len(y_orig))\n",
        "    grafikon(output,\"index\",\"orig\",\"orig\",\"pred\",\"pred\")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnlcf3ie3Mav"
      },
      "source": [
        ""
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpwk-qY7j5Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "6a39a4cd-90a7-4584-8de7-dcd574d9f2c2"
      },
      "source": [
        "backtest_train()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"7f1747bf-1781-4aee-a832-55118d5a0a45\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"7f1747bf-1781-4aee-a832-55118d5a0a45\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '7f1747bf-1781-4aee-a832-55118d5a0a45',\n",
              "                        [{\"line\": {\"color\": \"rgba(35,128,132,0.8)\"}, \"name\": \"orig\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509], \"xaxis\": \"x\", \"y\": [-1.0, -0.9827586206896551, -1.0, -1.0, -1.0, -1.0, -0.9827586206896551, -0.9827586206896551, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.9827586206896551, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.9827586206896551, -1.0, -1.0, -1.0, -1.0, -0.9827586206896551, -0.9827586206896551, -1.0, -1.0, -0.9827586206896551, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.9827586206896551, -0.9827586206896551, -0.9827586206896551, -0.9655172413793104, -0.9310344827586207, -0.9827586206896551, -0.9310344827586207, -0.8103448275862069, -0.7241379310344828, -0.603448275862069, -0.7931034482758621, -0.7586206896551724, -0.6896551724137931, -0.8620689655172413, -0.8793103448275862, -0.8275862068965517, -0.8793103448275862, -0.8275862068965517, -0.9137931034482758, -0.8103448275862069, -0.8620689655172413, -0.6896551724137931, -0.7758620689655172, -0.8448275862068966, -0.6206896551724138, -0.8275862068965517, -0.9137931034482758, -0.7758620689655172, -0.9655172413793104, -0.8103448275862069, -0.8103448275862069, -0.9482758620689655, -0.8793103448275862, -0.8793103448275862, -0.9310344827586207, -0.9137931034482758, -0.896551724137931, -0.8793103448275862, -0.8793103448275862, -0.9310344827586207, -0.8448275862068966, -0.7068965517241379, -0.8620689655172413, -0.6206896551724138, -0.6896551724137931, -0.6379310344827587, -0.7241379310344828, -0.4655172413793104, -0.5689655172413793, -0.5172413793103449, -0.5517241379310345, -0.6896551724137931, -0.5344827586206897, -0.8103448275862069, -0.3448275862068966, -0.5, -0.6379310344827587, -0.8103448275862069, -0.8275862068965517, -0.9137931034482758, -0.896551724137931, -0.9655172413793104, -0.9827586206896551, -0.9655172413793104, -0.9655172413793104, -0.9482758620689655, -0.9137931034482758, -0.9827586206896551, -0.9310344827586207, -0.9655172413793104, -0.9310344827586207, -1.0, -1.0, -1.0, -1.0, -0.9827586206896551, -0.9827586206896551, -0.9827586206896551, -0.9827586206896551, -0.9827586206896551, -0.9655172413793104, -0.9482758620689655, -0.9310344827586207, -0.896551724137931, -0.9655172413793104, -0.9655172413793104, -0.9137931034482758, -0.9827586206896551, -0.9827586206896551, -1.0, -1.0, -1.0, -1.0, -0.9655172413793104, -1.0, -0.9482758620689655, -1.0, -1.0, -1.0, -0.9655172413793104, -0.9655172413793104, -0.9482758620689655, -0.9482758620689655, -0.9482758620689655, -0.9827586206896551, -0.9655172413793104, -0.9482758620689655, -0.896551724137931, -0.9137931034482758, -0.9827586206896551, -0.9310344827586207, -0.9137931034482758, -0.8620689655172413, -0.9137931034482758, -0.9655172413793104, -0.9482758620689655, -0.9482758620689655, -0.9827586206896551, -0.896551724137931, -0.9310344827586207, -0.9827586206896551, -0.9655172413793104, -0.9482758620689655, -0.9827586206896551, -0.8620689655172413, -0.9310344827586207, -0.896551724137931, -0.8793103448275862, -0.9137931034482758, -0.8620689655172413, -0.896551724137931, -0.9137931034482758, -0.896551724137931, -0.896551724137931, -0.7758620689655172, -0.9655172413793104, -0.8275862068965517, -0.9482758620689655, -0.7931034482758621, -0.8793103448275862, -0.896551724137931, -0.9137931034482758, -0.896551724137931, -0.896551724137931, -0.896551724137931, -0.8620689655172413, -0.896551724137931, -0.8448275862068966, -0.7931034482758621, -0.6724137931034483, -0.8620689655172413, -0.7241379310344828, -0.6379310344827587, -0.896551724137931, -0.6206896551724138, -0.36206896551724144, -0.43103448275862066, -0.6896551724137931, 0.4310344827586208, 1.0, -0.4482758620689655, -0.8793103448275862, -0.8448275862068966, -0.8275862068965517, -0.9137931034482758, -0.8620689655172413, -0.8793103448275862, -0.8620689655172413, -0.8103448275862069, -0.896551724137931, -0.8793103448275862, -0.8793103448275862, -0.7586206896551724, -0.8793103448275862, -0.8448275862068966, -0.7758620689655172, -0.7241379310344828, -0.8793103448275862, -0.8448275862068966, -0.9655172413793104, -0.7758620689655172, -0.8620689655172413, -0.9482758620689655, -0.9137931034482758, -0.9310344827586207, -0.8620689655172413, -0.9655172413793104, -0.9482758620689655, -0.9137931034482758, -0.8793103448275862, -0.9482758620689655, -0.9137931034482758, -0.896551724137931, -0.9137931034482758, -0.9137931034482758, -0.9310344827586207, -1.0, -1.0, -1.0, -1.0, -1.0, -0.9655172413793104, -0.9310344827586207, -0.9310344827586207, -0.9482758620689655, -0.9482758620689655, -0.9137931034482758, -0.896551724137931, -0.7586206896551724, -0.9482758620689655, -0.8793103448275862, -0.8103448275862069, -0.9655172413793104, -0.896551724137931, -0.8620689655172413, -0.5689655172413793, -0.6379310344827587, -0.8275862068965517, -0.5172413793103449, -0.3275862068965517, -0.6551724137931034, -0.5862068965517242, -0.5172413793103449, -0.5517241379310345, -0.8620689655172413, -0.8448275862068966, -0.7931034482758621, -0.6896551724137931, -0.8448275862068966, -0.8448275862068966, -0.896551724137931, -0.896551724137931, -0.8620689655172413, -0.9137931034482758, -0.8793103448275862, -0.896551724137931, -0.9137931034482758, -0.9482758620689655, -0.9827586206896551, -1.0, -0.9827586206896551, -0.9655172413793104, -0.9482758620689655, -0.9655172413793104, -0.9655172413793104, -0.9655172413793104, -0.9655172413793104, -0.9655172413793104, -0.9310344827586207, -1.0, -0.896551724137931, -0.9482758620689655, -0.9655172413793104, -0.896551724137931, -0.9655172413793104, -0.8793103448275862, -0.9310344827586207, -0.896551724137931, -0.896551724137931, -0.9655172413793104, -0.7758620689655172, -0.8275862068965517, -0.9137931034482758, -0.9655172413793104, -1.0, -0.9827586206896551, -1.0, -0.7586206896551724, -0.896551724137931, -0.8275862068965517, -0.9137931034482758, -0.7931034482758621, -0.8448275862068966, -0.9137931034482758, -0.8103448275862069, -0.9655172413793104, -0.896551724137931, -0.8793103448275862, -0.896551724137931, -0.9137931034482758, -0.8448275862068966, -0.9137931034482758, -0.8620689655172413, -0.9482758620689655, -0.9310344827586207, -0.8103448275862069, -0.9137931034482758, -0.8620689655172413, -0.9310344827586207, -0.9482758620689655, -0.9827586206896551, -0.9655172413793104, -0.9482758620689655, -0.9310344827586207, -0.9827586206896551, -0.8620689655172413, -0.9137931034482758, -0.9482758620689655, -0.9655172413793104, -0.8793103448275862, -0.9827586206896551, -0.896551724137931, -0.8793103448275862, -0.9137931034482758, -0.9655172413793104, -0.896551724137931, -0.8103448275862069, -0.896551724137931, -0.9482758620689655, -0.8103448275862069, -0.8103448275862069, -0.9137931034482758, -0.9310344827586207, -0.8448275862068966, -0.603448275862069, -0.5172413793103449, -0.5517241379310345, -0.8793103448275862, -0.5, 0.0, -0.5517241379310345, -0.3448275862068966, -0.39655172413793105, -0.36206896551724144, -0.6551724137931034, -0.5, -0.5689655172413793, -0.603448275862069, -0.8448275862068966, -0.9482758620689655, -0.896551724137931, -0.896551724137931, -0.9482758620689655, -0.9827586206896551, -0.9482758620689655, -0.9827586206896551, -0.9827586206896551, -1.0, -0.9655172413793104, -0.9827586206896551, -0.9827586206896551, -1.0, -1.0, -0.9827586206896551, -1.0, -0.9482758620689655, -0.9482758620689655, -0.9827586206896551, -0.9137931034482758, -0.9655172413793104, -0.9137931034482758, -0.9137931034482758, -0.9137931034482758, -0.8448275862068966, -0.7068965517241379, -0.6724137931034483, -0.5689655172413793, -0.22413793103448276, -0.4137931034482759, 0.0862068965517242, -0.24137931034482762, -0.13793103448275867, -0.39655172413793105, -0.7241379310344828, -0.7241379310344828, -0.7758620689655172, -0.8448275862068966, -0.7413793103448276, -0.9310344827586207, -1.0, -0.9827586206896551, -0.8275862068965517, -0.8103448275862069, -0.5, -0.39655172413793105, -0.48275862068965514, -0.6551724137931034, -0.6379310344827587, -0.7931034482758621, -0.8448275862068966, -0.8103448275862069, -0.8448275862068966, -0.9137931034482758, -0.8103448275862069, -0.9482758620689655, -0.9137931034482758, -0.9137931034482758, -0.9310344827586207, -0.9310344827586207, -0.9827586206896551, -1.0, -0.9655172413793104, -0.9482758620689655, -0.9482758620689655, -0.9137931034482758, -0.9655172413793104, -0.9827586206896551, -0.9655172413793104, -1.0, -1.0, -0.9482758620689655, -0.9137931034482758, -0.9137931034482758, -0.8793103448275862, -0.9137931034482758, -0.9655172413793104, -0.9655172413793104, -0.9655172413793104, -1.0, -0.9655172413793104, -0.9827586206896551, -0.9827586206896551, -0.9655172413793104, -0.9655172413793104, -0.9482758620689655, -0.8448275862068966, -0.9137931034482758, -0.9137931034482758, -0.9310344827586207, -0.9310344827586207, -0.9827586206896551, -1.0, -1.0, -0.8275862068965517, -0.8448275862068966, -0.7068965517241379, -0.7241379310344828, -0.8103448275862069, -0.7931034482758621, -0.6724137931034483, -0.7413793103448276, -0.7931034482758621, -0.7931034482758621, -0.7241379310344828, -0.8448275862068966, -0.9310344827586207, -0.8448275862068966, -0.896551724137931, -0.8620689655172413, -0.9310344827586207, -0.9655172413793104, -0.8793103448275862, -0.896551724137931, -0.9137931034482758, -0.8620689655172413, -0.9827586206896551, -0.9827586206896551], \"yaxis\": \"y\"}, {\"line\": {\"color\": \"rgba(193,99,99,0.8)\"}, \"name\": \"pred\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509], \"xaxis\": \"x\", \"y\": [-0.8909487724304199, -0.953510582447052, -0.9282935857772827, -0.956459641456604, -0.9532172083854675, -0.9717664122581482, -0.9765874147415161, -0.9716459512710571, -0.9658734202384949, -0.9225203990936279, -0.9596279859542847, -0.9777826070785522, -0.9857730269432068, -0.9770282506942749, -0.9652108550071716, -0.9748912453651428, -0.9821605682373047, -0.9767873287200928, -0.9859791994094849, -0.9637050032615662, -0.9652401804924011, -0.9710866808891296, -0.9641684293746948, -0.9575715065002441, -0.9604461193084717, -0.9304031133651733, -0.9543030858039856, -0.9462359547615051, -0.9206176400184631, -0.9310750365257263, -0.9298320412635803, -0.9388646483421326, -0.9451618790626526, -0.9455260634422302, -0.9723397493362427, -0.9576664566993713, -0.9543392658233643, -0.9715847969055176, -0.9696445465087891, -0.9620397090911865, -0.9608520269393921, -0.9536074995994568, -0.9683777093887329, -0.9461920857429504, -0.9218763709068298, -0.9214476346969604, -0.9277777075767517, -0.9377477765083313, -0.8833361268043518, -0.9084151983261108, -0.8843112587928772, -0.937554657459259, -0.9576740264892578, -0.9668980240821838, -0.9500579833984375, -0.9479010701179504, -0.9345411658287048, -0.972818911075592, -0.960539698600769, -0.9580699801445007, -0.9813342690467834, -0.9867361187934875, -0.9781576991081238, -0.9761368036270142, -0.9711303114891052, -0.971272349357605, -0.9778826832771301, -0.9797964096069336, -0.9798151850700378, -0.9817745685577393, -0.9812566637992859, -0.9711265563964844, -0.9731312394142151, -0.9800190925598145, -0.9737054109573364, -0.9807698726654053, -0.9838277697563171, -0.9651691317558289, -0.9416948556900024, -0.9699934124946594, -0.9535402655601501, -0.9579218626022339, -0.9318656921386719, -0.9773067831993103, -0.9729655385017395, -0.9720277190208435, -0.9536210298538208, -0.9612249732017517, -0.9656575918197632, -0.969666063785553, -0.9658516049385071, -0.9817115664482117, -0.9533843398094177, -0.9282312393188477, -0.9672239422798157, -0.94349604845047, -0.9661634564399719, -0.9078238606452942, -0.9607018232345581, -0.9239742159843445, -0.9031826257705688, -0.8471648097038269, -0.9075271487236023, -0.8856456875801086, -0.9338576793670654, -0.9747692346572876, -0.9578362703323364, -0.9532698392868042, -0.9429536461830139, -0.9735036492347717, -0.9794197082519531, -0.9832985997200012, -0.9843431115150452, -0.9514317512512207, -0.9783648252487183, -0.9801244735717773, -0.975810170173645, -0.9734858274459839, -0.968143880367279, -0.9680980443954468, -0.9732296466827393, -0.9842007756233215, -0.9725638628005981, -0.9674069285392761, -0.9487526416778564, -0.9760702848434448, -0.9803226590156555, -0.9809799790382385, -0.9625941514968872, -0.9724328517913818, -0.9835286736488342, -0.9518893957138062, -0.9455182552337646, -0.977724552154541, -0.9851661324501038, -0.9484952688217163, -0.9618805646896362, -0.9740543365478516, -0.9767066836357117, -0.9486697316169739, -0.9789509773254395, -0.9645715355873108, -0.9696103930473328, -0.9821321368217468, -0.9729114770889282, -0.9348657131195068, -0.9564065337181091, -0.9532012939453125, -0.9348251819610596, -0.954783022403717, -0.9527549743652344, -0.9047260284423828, -0.886186957359314, -0.9121997356414795, -0.9233596324920654, -0.8724495768547058, -0.9135509729385376, -0.9481866955757141, -0.9391657114028931, -0.9504575133323669, -0.9564564824104309, -0.9753950238227844, -0.9512824416160583, -0.9577406644821167, -0.9294255971908569, -0.9777355790138245, -0.9778791069984436, -0.9671487808227539, -0.9811676740646362, -0.9855037927627563, -0.9636395573616028, -0.9779473543167114, -0.9765885472297668, -0.9807089567184448, -0.9782466292381287, -0.9840617179870605, -0.9753303527832031, -0.9607179760932922, -0.9700226783752441, -0.955267071723938, -0.9430512189865112, -0.9750815629959106, -0.9835323095321655, -0.9772499203681946, -0.968416690826416, -0.9722038507461548, -0.9617224931716919, -0.9773817658424377, -0.9702991247177124, -0.9643955230712891, -0.9780502915382385, -0.9617666006088257, -0.9678192138671875, -0.9848860502243042, -0.9716284275054932, -0.9499329924583435, -0.9800699949264526, -0.9377767443656921, -0.9544907212257385, -0.945399820804596, -0.9354070425033569, -0.9493045210838318, -0.9224491119384766, -0.938056468963623, -0.907691478729248, -0.9427424073219299, -0.9407625198364258, -0.9225634932518005, -0.9309691190719604, -0.9110578298568726, -0.965013861656189, -0.9636129140853882, -0.9565859436988831, -0.9432052969932556, -0.9633938670158386, -0.9676547646522522, -0.9838489890098572, -0.9772871136665344, -0.9573164582252502, -0.9589335918426514, -0.9846279621124268, -0.9864426255226135, -0.9696934223175049, -0.971992015838623, -0.9698337912559509, -0.9745906591415405, -0.9700131416320801, -0.9795939922332764, -0.9761677384376526, -0.9802671670913696, -0.9838718175888062, -0.9809560179710388, -0.9791017174720764, -0.9808156490325928, -0.9906880855560303, -0.9829245805740356, -0.960966169834137, -0.966127872467041, -0.9815967082977295, -0.9637795686721802, -0.959312915802002, -0.9709150195121765, -0.9771357774734497, -0.987069308757782, -0.9742169976234436, -0.9655580520629883, -0.9556068181991577, -0.9654578566551208, -0.9540166258811951, -0.9621294140815735, -0.9670282602310181, -0.961726188659668, -0.9698820114135742, -0.9625892639160156, -0.9419196844100952, -0.9291163086891174, -0.9430758953094482, -0.9065624475479126, -0.9287045001983643, -0.9235023260116577, -0.9261955618858337, -0.9603128433227539, -0.9448903799057007, -0.9691697955131531, -0.9421936273574829, -0.9624667763710022, -0.9638309478759766, -0.9684554934501648, -0.9765229821205139, -0.9843060374259949, -0.9851124286651611, -0.9744635224342346, -0.9695931077003479, -0.9688451886177063, -0.9836220741271973, -0.982069194316864, -0.9811660647392273, -0.9862316846847534, -0.9722392559051514, -0.9813835024833679, -0.9849246144294739, -0.9473631978034973, -0.9552269577980042, -0.9845371246337891, -0.9743400812149048, -0.9697460532188416, -0.9593241810798645, -0.9736328721046448, -0.9634433388710022, -0.950421929359436, -0.9734518527984619, -0.9725976586341858, -0.9525413513183594, -0.9786272048950195, -0.9680227041244507, -0.9639488458633423, -0.9737611413002014, -0.9760424494743347, -0.9369297623634338, -0.9628254771232605, -0.9182325005531311, -0.9436571002006531, -0.9699103236198425, -0.9351393580436707, -0.8854773044586182, -0.891913115978241, -0.9516740441322327, -0.9197163581848145, -0.941120445728302, -0.9217304587364197, -0.962026834487915, -0.9390156865119934, -0.9555321335792542, -0.944473385810852, -0.948894739151001, -0.9537010788917542, -0.9538260698318481, -0.9528871178627014, -0.9698982834815979, -0.9675771594047546, -0.9821764230728149, -0.9710230231285095, -0.9757165908813477, -0.9813423752784729, -0.9763764142990112, -0.9812880754470825, -0.9936149716377258, -0.982755184173584, -0.962857723236084, -0.9740762710571289, -0.9871614575386047, -0.9715295433998108, -0.9826340675354004, -0.9737837314605713, -0.9801903367042542, -0.9763941764831543, -0.9522894024848938, -0.9463488459587097, -0.9728856086730957, -0.9614728689193726, -0.979985237121582, -0.9793702960014343, -0.9722640514373779, -0.9461963772773743, -0.9748565554618835, -0.9767950773239136, -0.9725721478462219, -0.9518890976905823, -0.9817248582839966, -0.9471650719642639, -0.9639961123466492, -0.959426760673523, -0.9643312096595764, -0.9489169120788574, -0.9285330772399902, -0.9736207127571106, -0.9114125370979309, -0.9227128028869629, -0.9248727560043335, -0.9341785311698914, -0.9348623752593994, -0.9298054575920105, -0.9190528988838196, -0.9348191618919373, -0.9488902688026428, -0.9236959218978882, -0.9395539164543152, -0.9687580466270447, -0.975387692451477, -0.9572733640670776, -0.9488425254821777, -0.9862716197967529, -0.975808322429657, -0.9699890613555908, -0.9850558042526245, -0.9811984300613403, -0.9641242027282715, -0.9827995896339417, -0.9822044968605042, -0.9738750457763672, -0.9656680226325989, -0.9661853313446045, -0.9847955107688904, -0.9679720401763916, -0.9683724045753479, -0.9701329469680786, -0.9742453694343567, -0.9482486844062805, -0.9620323777198792, -0.9615360498428345, -0.9667475819587708, -0.9877185821533203, -0.9632372260093689, -0.9581547975540161, -0.9375701546669006, -0.978349506855011, -0.9648156762123108, -0.9555329084396362, -0.9591222405433655, -0.9820603728294373, -0.9630945920944214, -0.9536343812942505, -0.9388718008995056, -0.9703502655029297, -0.952462911605835, -0.9300040602684021, -0.9260190725326538, -0.9212982654571533, -0.8897533416748047, -0.9580298066139221, -0.9108180403709412, -0.9296536445617676, -0.9339800477027893, -0.9649398922920227, -0.9210535287857056, -0.9442622661590576, -0.9428994655609131, -0.9670714735984802, -0.9598668813705444, -0.9814162254333496, -0.9715932011604309, -0.9676956534385681, -0.9745200276374817, -0.9724456071853638, -0.9633324146270752, -0.9792879223823547, -0.9717452526092529, -0.9560609459877014, -0.9837047457695007, -0.9830196499824524, -0.9789655208587646, -0.9823383688926697, -0.9774563908576965, -0.9663127660751343, -0.9762330651283264, -0.976020097732544, -0.9742345809936523, -0.9745463728904724, -0.9738133549690247, -0.9660782814025879, -0.9464715719223022, -0.9701208472251892, -0.9445571303367615, -0.9401370882987976, -0.9648712277412415, -0.9725070595741272, -0.9658902287483215, -0.9739202260971069, -0.9802194237709045, -0.9538497924804688, -0.9499534964561462, -0.9736821055412292, -0.9654337763786316, -0.9657877683639526, -0.9665292501449585, -0.9691387414932251, -0.973859429359436, -0.9703053832054138, -0.9514881372451782, -0.9682999849319458, -0.9413104057312012, -0.9469829201698303, -0.9227637052536011, -0.9439565539360046, -0.947490930557251, -0.9445042014122009, -0.9522694945335388, -0.9503300189971924, -0.9720358848571777, -0.9465286135673523, -0.9711336493492126, -0.9821299910545349, -0.9579206705093384, -0.9645267128944397, -0.9868004322052002, -0.9792796969413757, -0.9596113562583923, -0.9693374633789062, -0.9836938381195068, -0.9828938841819763, -0.9709644913673401, -0.9782593250274658, -0.9847331047058105, -0.9834424257278442, -0.9941197633743286, -0.9764992594718933, -0.9837291836738586, -0.9838663339614868, -0.9725108742713928, -0.9653333425521851, -0.9816086292266846, -0.9724504947662354, -0.9709293842315674, -0.9675065279006958, -0.9672441482543945, -0.9730424880981445, -0.974017322063446, -0.9793453216552734, -0.9776380062103271, -0.9824466109275818, -0.9794782996177673, -0.978948712348938, -0.9860840439796448, -0.9853689074516296, -0.9752586483955383, -0.9819215536117554, -0.9798046350479126, -0.983525812625885, -0.9663869738578796, -0.9488767385482788], \"yaxis\": \"y\"}],\n",
              "                        {\"autosize\": false, \"height\": 600, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"orig--pred\"}, \"width\": 1200, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0]}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7f1747bf-1781-4aee-a832-55118d5a0a45');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VbgSLW-AHnF"
      },
      "source": [
        "X_a,y_a=build_training_data(X2,y2,1,1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXPvnU0FBcHb",
        "outputId": "c31f2c4f-99a8-449f-e6e3-5ef6a6f36864"
      },
      "source": [
        "X_a[0]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.1625714 ,  0.29404285, 25.41      , 23.59      , 25.3       ,\n",
              "       22.03428571, 34.15      , 19.95      , 43.19      , 92.41857143,\n",
              "       25.41      , 16.65142857,  8.92857143, 26.4       , 10.775     ,\n",
              "       32.5       , 20.7       ,  3.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of_tu1iwAonD",
        "outputId": "16300ee1-db30-4af2-a40b-442ff54a058e"
      },
      "source": [
        "clear_session() \n",
        "dropout_1_rate=0.15\n",
        "input_size=len(X_a[0]) # timesteps\n",
        "output_size=len(X_a[0]) # features\n",
        "learning_rate=0.00001\n",
        "\n",
        "inputs=Input(shape=(input_size))\n",
        "o1=Dense(1500,activation=\"relu\",kernel_initializer=\"HeNormal\")(inputs)\n",
        "d1=Dropout(dropout_1_rate,)(o1)\n",
        "o2=Dense(9,activation=\"relu\",kernel_initializer=\"HeNormal\")(d1)\n",
        "d2=Dropout(dropout_1_rate)(o2)\n",
        "\n",
        "o3=Dense(1500,activation=\"relu\",kernel_initializer=\"HeNormal\")(d2)\n",
        "#d3=Dropout(dropout_1_rate)(o3)\n",
        "\n",
        "result=Dense(input_size,activation=\"tanh\",)(o3)\n",
        "#dropout1=Dropout(rate=dropout_1_rate)(layer2)\n",
        "#result=Dense(output_size,activation=\"sigmoid\")(dropout1)\n",
        "\n",
        "model_a = Model(inputs=inputs, outputs=result)\n",
        "model_a.compile(optimizer=Adamax(learning_rate=learning_rate,), loss=\"mae\")  #RMSprop\n",
        "model_a.summary()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 18)]              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1500)              28500     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1500)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9)                 13509     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 9)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1500)              15000     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 18)                27018     \n",
            "=================================================================\n",
            "Total params: 84,027\n",
            "Trainable params: 84,027\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dxJMYIKA2yA"
      },
      "source": [
        ""
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmSvtPplA3B7"
      },
      "source": [
        "fname=\"./model_a\"\n",
        "callbacks = [#callback_LR,\n",
        "       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch}\"+\"_{loss:.5f}_{val_loss:.5f}_.hdf5\", monitor='val_loss',\n",
        "                        verbose=2, save_best_only=True, mode='min',)]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf9-gjiJA3B9",
        "outputId": "3296a678-4ba3-43b1-b295-785edd97ff57"
      },
      "source": [
        "\n",
        "from keras import backend \n",
        "backend.set_value(model_a.optimizer.learning_rate, 0.00001) # 0.0001--\n",
        "\n",
        "model_a.optimizer.iterations"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'iter:0' shape=() dtype=int64, numpy=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeXwAI4QC1V5"
      },
      "source": [
        "x_a_scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "Xa_scale = pd.DataFrame(x_a_scaler.fit_transform(X_a))\n",
        "\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "wZD2A-9VDL2M",
        "outputId": "a360bc5a-e6b1-4cd8-ff23-d8fcdf7d1678"
      },
      "source": [
        "Xa_scale"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.542077</td>\n",
              "      <td>-0.057274</td>\n",
              "      <td>-0.758953</td>\n",
              "      <td>-0.398898</td>\n",
              "      <td>-0.114667</td>\n",
              "      <td>0.218862</td>\n",
              "      <td>0.042857</td>\n",
              "      <td>0.362637</td>\n",
              "      <td>-0.761401</td>\n",
              "      <td>0.696669</td>\n",
              "      <td>-0.758953</td>\n",
              "      <td>0.087425</td>\n",
              "      <td>-0.153132</td>\n",
              "      <td>0.063830</td>\n",
              "      <td>5.188679e-02</td>\n",
              "      <td>-0.603306</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>-0.988956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.136292</td>\n",
              "      <td>-0.186795</td>\n",
              "      <td>-0.425034</td>\n",
              "      <td>-0.429096</td>\n",
              "      <td>-0.120000</td>\n",
              "      <td>0.260550</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.076923</td>\n",
              "      <td>-0.745877</td>\n",
              "      <td>0.753639</td>\n",
              "      <td>-0.425034</td>\n",
              "      <td>0.138067</td>\n",
              "      <td>0.071926</td>\n",
              "      <td>0.170213</td>\n",
              "      <td>2.012579e-01</td>\n",
              "      <td>-0.355372</td>\n",
              "      <td>0.284211</td>\n",
              "      <td>-0.795325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.482239</td>\n",
              "      <td>-0.639174</td>\n",
              "      <td>-0.473320</td>\n",
              "      <td>-0.491532</td>\n",
              "      <td>-0.377778</td>\n",
              "      <td>0.323766</td>\n",
              "      <td>-0.357143</td>\n",
              "      <td>0.252747</td>\n",
              "      <td>-0.642184</td>\n",
              "      <td>0.864712</td>\n",
              "      <td>-0.473320</td>\n",
              "      <td>0.199658</td>\n",
              "      <td>-0.403712</td>\n",
              "      <td>0.148936</td>\n",
              "      <td>1.823899e-01</td>\n",
              "      <td>-0.520661</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>-0.859746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.425959</td>\n",
              "      <td>-0.360939</td>\n",
              "      <td>-0.946877</td>\n",
              "      <td>-0.793920</td>\n",
              "      <td>-0.667556</td>\n",
              "      <td>-0.352127</td>\n",
              "      <td>-0.485714</td>\n",
              "      <td>-0.626374</td>\n",
              "      <td>-0.867635</td>\n",
              "      <td>0.442679</td>\n",
              "      <td>-0.946877</td>\n",
              "      <td>-0.444311</td>\n",
              "      <td>-0.122970</td>\n",
              "      <td>-0.070922</td>\n",
              "      <td>6.289308e-03</td>\n",
              "      <td>-0.768595</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.889564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.245424</td>\n",
              "      <td>0.004855</td>\n",
              "      <td>-0.404639</td>\n",
              "      <td>-0.486635</td>\n",
              "      <td>-0.317333</td>\n",
              "      <td>-0.074663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>-0.824324</td>\n",
              "      <td>0.488102</td>\n",
              "      <td>-0.404639</td>\n",
              "      <td>-0.201711</td>\n",
              "      <td>-0.060325</td>\n",
              "      <td>0.106383</td>\n",
              "      <td>1.849057e-01</td>\n",
              "      <td>-0.471074</td>\n",
              "      <td>-0.073684</td>\n",
              "      <td>-0.985275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>-0.537136</td>\n",
              "      <td>-0.760705</td>\n",
              "      <td>-0.436513</td>\n",
              "      <td>-0.245052</td>\n",
              "      <td>-0.242667</td>\n",
              "      <td>0.590637</td>\n",
              "      <td>-0.142857</td>\n",
              "      <td>0.560440</td>\n",
              "      <td>-0.517720</td>\n",
              "      <td>0.942959</td>\n",
              "      <td>-0.436513</td>\n",
              "      <td>0.504192</td>\n",
              "      <td>-0.598608</td>\n",
              "      <td>0.276596</td>\n",
              "      <td>-1.886792e-02</td>\n",
              "      <td>-0.404959</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>-0.882201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>-0.033176</td>\n",
              "      <td>-0.251230</td>\n",
              "      <td>-0.989091</td>\n",
              "      <td>-0.139359</td>\n",
              "      <td>-0.050667</td>\n",
              "      <td>-0.249274</td>\n",
              "      <td>0.242857</td>\n",
              "      <td>-0.296703</td>\n",
              "      <td>-0.951385</td>\n",
              "      <td>0.039194</td>\n",
              "      <td>-0.989091</td>\n",
              "      <td>-0.330026</td>\n",
              "      <td>0.222738</td>\n",
              "      <td>-0.099291</td>\n",
              "      <td>-2.452830e-01</td>\n",
              "      <td>-0.355372</td>\n",
              "      <td>0.115789</td>\n",
              "      <td>-0.990797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>0.271733</td>\n",
              "      <td>-0.146457</td>\n",
              "      <td>-0.475407</td>\n",
              "      <td>0.342175</td>\n",
              "      <td>0.463111</td>\n",
              "      <td>0.611481</td>\n",
              "      <td>0.385714</td>\n",
              "      <td>0.670330</td>\n",
              "      <td>-0.751402</td>\n",
              "      <td>0.517707</td>\n",
              "      <td>-0.475407</td>\n",
              "      <td>0.526775</td>\n",
              "      <td>-0.011601</td>\n",
              "      <td>0.539007</td>\n",
              "      <td>2.704403e-01</td>\n",
              "      <td>-0.123967</td>\n",
              "      <td>0.621053</td>\n",
              "      <td>-0.900607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>-0.553236</td>\n",
              "      <td>-0.571946</td>\n",
              "      <td>-0.179718</td>\n",
              "      <td>0.055295</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.522296</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.144104</td>\n",
              "      <td>0.656565</td>\n",
              "      <td>-0.179718</td>\n",
              "      <td>0.427203</td>\n",
              "      <td>-0.389791</td>\n",
              "      <td>0.283688</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>-0.239669</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>-0.865268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>-0.377695</td>\n",
              "      <td>-0.293743</td>\n",
              "      <td>-0.440877</td>\n",
              "      <td>-0.439298</td>\n",
              "      <td>-0.328000</td>\n",
              "      <td>0.294721</td>\n",
              "      <td>-0.214286</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>-0.720465</td>\n",
              "      <td>0.787864</td>\n",
              "      <td>-0.440877</td>\n",
              "      <td>0.172968</td>\n",
              "      <td>-0.385151</td>\n",
              "      <td>-0.361702</td>\n",
              "      <td>-6.792453e-01</td>\n",
              "      <td>-0.652893</td>\n",
              "      <td>-0.052632</td>\n",
              "      <td>-0.972759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>518 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2   ...        15        16        17\n",
              "0   -0.542077 -0.057274 -0.758953  ... -0.603306  0.263158 -0.988956\n",
              "1   -0.136292 -0.186795 -0.425034  ... -0.355372  0.284211 -0.795325\n",
              "2   -0.482239 -0.639174 -0.473320  ... -0.520661  0.263158 -0.859746\n",
              "3   -0.425959 -0.360939 -0.946877  ... -0.768595 -1.000000 -0.889564\n",
              "4    0.245424  0.004855 -0.404639  ... -0.471074 -0.073684 -0.985275\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "513 -0.537136 -0.760705 -0.436513  ... -0.404959  0.368421 -0.882201\n",
              "514 -0.033176 -0.251230 -0.989091  ... -0.355372  0.115789 -0.990797\n",
              "515  0.271733 -0.146457 -0.475407  ... -0.123967  0.621053 -0.900607\n",
              "516 -0.553236 -0.571946 -0.179718  ... -0.239669  0.473684 -0.865268\n",
              "517 -0.377695 -0.293743 -0.440877  ... -0.652893 -0.052632 -0.972759\n",
              "\n",
              "[518 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDIHvtnlA3B-",
        "outputId": "a6469f67-bb70-403c-acac-f0237f82216f"
      },
      "source": [
        "history = model_a.fit(\n",
        "    x=Xa_scale,\n",
        "    y=Xa_scale,\n",
        "    validation_split=0.1,\n",
        "    epochs=1500,\n",
        "    batch_size=1,\n",
        "    callbacks=callbacks,shuffle=True\n",
        "   )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1500\n",
            "466/466 [==============================] - 2s 2ms/step - loss: 0.4998 - val_loss: 0.3943\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.39427, saving model to ./model_a_1_0.46547_0.39427_.hdf5\n",
            "Epoch 2/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.3878 - val_loss: 0.3201\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.39427 to 0.32005, saving model to ./model_a_2_0.37283_0.32005_.hdf5\n",
            "Epoch 3/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.3424 - val_loss: 0.2769\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.32005 to 0.27686, saving model to ./model_a_3_0.32962_0.27686_.hdf5\n",
            "Epoch 4/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.3111 - val_loss: 0.2534\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.27686 to 0.25336, saving model to ./model_a_4_0.30388_0.25336_.hdf5\n",
            "Epoch 5/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2813 - val_loss: 0.2428\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.25336 to 0.24285, saving model to ./model_a_5_0.28425_0.24285_.hdf5\n",
            "Epoch 6/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2774 - val_loss: 0.2335\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.24285 to 0.23350, saving model to ./model_a_6_0.27756_0.23350_.hdf5\n",
            "Epoch 7/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2755 - val_loss: 0.2288\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.23350 to 0.22885, saving model to ./model_a_7_0.27355_0.22885_.hdf5\n",
            "Epoch 8/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2657 - val_loss: 0.2245\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.22885 to 0.22449, saving model to ./model_a_8_0.26449_0.22449_.hdf5\n",
            "Epoch 9/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2652 - val_loss: 0.2223\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.22449 to 0.22232, saving model to ./model_a_9_0.26190_0.22232_.hdf5\n",
            "Epoch 10/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2535 - val_loss: 0.2174\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.22232 to 0.21742, saving model to ./model_a_10_0.25357_0.21742_.hdf5\n",
            "Epoch 11/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2571 - val_loss: 0.2139\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.21742 to 0.21391, saving model to ./model_a_11_0.25095_0.21391_.hdf5\n",
            "Epoch 12/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2527 - val_loss: 0.2105\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.21391 to 0.21049, saving model to ./model_a_12_0.24944_0.21049_.hdf5\n",
            "Epoch 13/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2513 - val_loss: 0.2081\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.21049 to 0.20807, saving model to ./model_a_13_0.24359_0.20807_.hdf5\n",
            "Epoch 14/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2451 - val_loss: 0.2067\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.20807 to 0.20668, saving model to ./model_a_14_0.24237_0.20668_.hdf5\n",
            "Epoch 15/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2383 - val_loss: 0.2023\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.20668 to 0.20228, saving model to ./model_a_15_0.23840_0.20228_.hdf5\n",
            "Epoch 16/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2349 - val_loss: 0.2003\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.20228 to 0.20029, saving model to ./model_a_16_0.23700_0.20029_.hdf5\n",
            "Epoch 17/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2264 - val_loss: 0.1970\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.20029 to 0.19696, saving model to ./model_a_17_0.22954_0.19696_.hdf5\n",
            "Epoch 18/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2325 - val_loss: 0.1939\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.19696 to 0.19386, saving model to ./model_a_18_0.23126_0.19386_.hdf5\n",
            "Epoch 19/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2254 - val_loss: 0.1905\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.19386 to 0.19053, saving model to ./model_a_19_0.22581_0.19053_.hdf5\n",
            "Epoch 20/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2217 - val_loss: 0.1868\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.19053 to 0.18679, saving model to ./model_a_20_0.22076_0.18679_.hdf5\n",
            "Epoch 21/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2151 - val_loss: 0.1846\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.18679 to 0.18463, saving model to ./model_a_21_0.22115_0.18463_.hdf5\n",
            "Epoch 22/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2218 - val_loss: 0.1824\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.18463 to 0.18241, saving model to ./model_a_22_0.22018_0.18241_.hdf5\n",
            "Epoch 23/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2171 - val_loss: 0.1764\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.18241 to 0.17638, saving model to ./model_a_23_0.21407_0.17638_.hdf5\n",
            "Epoch 24/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2156 - val_loss: 0.1741\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.17638 to 0.17406, saving model to ./model_a_24_0.21260_0.17406_.hdf5\n",
            "Epoch 25/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2146 - val_loss: 0.1720\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.17406 to 0.17200, saving model to ./model_a_25_0.20972_0.17200_.hdf5\n",
            "Epoch 26/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2061 - val_loss: 0.1703\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.17200 to 0.17032, saving model to ./model_a_26_0.20693_0.17032_.hdf5\n",
            "Epoch 27/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2040 - val_loss: 0.1678\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.17032 to 0.16782, saving model to ./model_a_27_0.20386_0.16782_.hdf5\n",
            "Epoch 28/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2071 - val_loss: 0.1651\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.16782 to 0.16508, saving model to ./model_a_28_0.20503_0.16508_.hdf5\n",
            "Epoch 29/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1956 - val_loss: 0.1626\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.16508 to 0.16261, saving model to ./model_a_29_0.19940_0.16261_.hdf5\n",
            "Epoch 30/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2032 - val_loss: 0.1617\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.16261 to 0.16174, saving model to ./model_a_30_0.20423_0.16174_.hdf5\n",
            "Epoch 31/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2012 - val_loss: 0.1614\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.16174 to 0.16136, saving model to ./model_a_31_0.19971_0.16136_.hdf5\n",
            "Epoch 32/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1912 - val_loss: 0.1603\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.16136 to 0.16031, saving model to ./model_a_32_0.19398_0.16031_.hdf5\n",
            "Epoch 33/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1966 - val_loss: 0.1588\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.16031 to 0.15876, saving model to ./model_a_33_0.19823_0.15876_.hdf5\n",
            "Epoch 34/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1943 - val_loss: 0.1580\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.15876 to 0.15798, saving model to ./model_a_34_0.19681_0.15798_.hdf5\n",
            "Epoch 35/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2017 - val_loss: 0.1561\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.15798 to 0.15612, saving model to ./model_a_35_0.19825_0.15612_.hdf5\n",
            "Epoch 36/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.2005 - val_loss: 0.1574\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.15612\n",
            "Epoch 37/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1856 - val_loss: 0.1571\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.15612\n",
            "Epoch 38/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1961 - val_loss: 0.1544\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.15612 to 0.15441, saving model to ./model_a_38_0.19483_0.15441_.hdf5\n",
            "Epoch 39/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1895 - val_loss: 0.1532\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.15441 to 0.15318, saving model to ./model_a_39_0.18923_0.15318_.hdf5\n",
            "Epoch 40/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1900 - val_loss: 0.1518\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.15318 to 0.15180, saving model to ./model_a_40_0.19098_0.15180_.hdf5\n",
            "Epoch 41/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1939 - val_loss: 0.1510\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.15180 to 0.15099, saving model to ./model_a_41_0.19164_0.15099_.hdf5\n",
            "Epoch 42/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1927 - val_loss: 0.1534\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.15099\n",
            "Epoch 43/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1884 - val_loss: 0.1518\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.15099\n",
            "Epoch 44/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1876 - val_loss: 0.1509\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.15099 to 0.15094, saving model to ./model_a_44_0.18580_0.15094_.hdf5\n",
            "Epoch 45/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1884 - val_loss: 0.1518\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.15094\n",
            "Epoch 46/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1892 - val_loss: 0.1514\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.15094\n",
            "Epoch 47/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1864 - val_loss: 0.1476\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.15094 to 0.14760, saving model to ./model_a_47_0.18668_0.14760_.hdf5\n",
            "Epoch 48/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1865 - val_loss: 0.1474\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.14760 to 0.14745, saving model to ./model_a_48_0.18814_0.14745_.hdf5\n",
            "Epoch 49/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1863 - val_loss: 0.1453\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.14745 to 0.14533, saving model to ./model_a_49_0.18348_0.14533_.hdf5\n",
            "Epoch 50/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1862 - val_loss: 0.1486\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.14533\n",
            "Epoch 51/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1797 - val_loss: 0.1470\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.14533\n",
            "Epoch 52/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1901 - val_loss: 0.1477\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.14533\n",
            "Epoch 53/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1841 - val_loss: 0.1433\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.14533 to 0.14332, saving model to ./model_a_53_0.18293_0.14332_.hdf5\n",
            "Epoch 54/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1891 - val_loss: 0.1490\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.14332\n",
            "Epoch 55/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1801 - val_loss: 0.1477\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.14332\n",
            "Epoch 56/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1808 - val_loss: 0.1508\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.14332\n",
            "Epoch 57/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1852 - val_loss: 0.1470\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.14332\n",
            "Epoch 58/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1844 - val_loss: 0.1461\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.14332\n",
            "Epoch 59/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1790 - val_loss: 0.1453\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.14332\n",
            "Epoch 60/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1798 - val_loss: 0.1466\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.14332\n",
            "Epoch 61/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1761 - val_loss: 0.1460\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.14332\n",
            "Epoch 62/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1846 - val_loss: 0.1432\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.14332 to 0.14323, saving model to ./model_a_62_0.18067_0.14323_.hdf5\n",
            "Epoch 63/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1783 - val_loss: 0.1459\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.14323\n",
            "Epoch 64/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1817 - val_loss: 0.1473\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.14323\n",
            "Epoch 65/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1755 - val_loss: 0.1434\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.14323\n",
            "Epoch 66/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1789 - val_loss: 0.1444\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.14323\n",
            "Epoch 67/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1769 - val_loss: 0.1440\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.14323\n",
            "Epoch 68/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1750 - val_loss: 0.1430\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.14323 to 0.14300, saving model to ./model_a_68_0.17926_0.14300_.hdf5\n",
            "Epoch 69/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1798 - val_loss: 0.1431\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.14300\n",
            "Epoch 70/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1795 - val_loss: 0.1458\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.14300\n",
            "Epoch 71/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1816 - val_loss: 0.1454\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.14300\n",
            "Epoch 72/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1806 - val_loss: 0.1433\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.14300\n",
            "Epoch 73/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1793 - val_loss: 0.1455\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.14300\n",
            "Epoch 74/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1773 - val_loss: 0.1440\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.14300\n",
            "Epoch 75/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1748 - val_loss: 0.1439\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.14300\n",
            "Epoch 76/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1726 - val_loss: 0.1410\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.14300 to 0.14100, saving model to ./model_a_76_0.17351_0.14100_.hdf5\n",
            "Epoch 77/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1847 - val_loss: 0.1404\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.14100 to 0.14038, saving model to ./model_a_77_0.17988_0.14038_.hdf5\n",
            "Epoch 78/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1742 - val_loss: 0.1428\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.14038\n",
            "Epoch 79/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1695 - val_loss: 0.1406\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.14038\n",
            "Epoch 80/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1665 - val_loss: 0.1436\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.14038\n",
            "Epoch 81/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1726 - val_loss: 0.1433\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.14038\n",
            "Epoch 82/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1718 - val_loss: 0.1449\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.14038\n",
            "Epoch 83/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1777 - val_loss: 0.1455\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.14038\n",
            "Epoch 84/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1672 - val_loss: 0.1440\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.14038\n",
            "Epoch 85/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1701 - val_loss: 0.1434\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.14038\n",
            "Epoch 86/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1709 - val_loss: 0.1406\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.14038\n",
            "Epoch 87/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1714 - val_loss: 0.1450\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.14038\n",
            "Epoch 88/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1750 - val_loss: 0.1468\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.14038\n",
            "Epoch 89/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1774 - val_loss: 0.1424\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.14038\n",
            "Epoch 90/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1737 - val_loss: 0.1427\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.14038\n",
            "Epoch 91/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1744 - val_loss: 0.1395\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.14038 to 0.13952, saving model to ./model_a_91_0.17227_0.13952_.hdf5\n",
            "Epoch 92/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1696 - val_loss: 0.1417\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.13952\n",
            "Epoch 93/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1778 - val_loss: 0.1409\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.13952\n",
            "Epoch 94/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1732 - val_loss: 0.1433\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.13952\n",
            "Epoch 95/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1687 - val_loss: 0.1385\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.13952 to 0.13852, saving model to ./model_a_95_0.17247_0.13852_.hdf5\n",
            "Epoch 96/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1717 - val_loss: 0.1434\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.13852\n",
            "Epoch 97/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1657 - val_loss: 0.1432\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.13852\n",
            "Epoch 98/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1693 - val_loss: 0.1375\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.13852 to 0.13753, saving model to ./model_a_98_0.17005_0.13753_.hdf5\n",
            "Epoch 99/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1647 - val_loss: 0.1419\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.13753\n",
            "Epoch 100/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1679 - val_loss: 0.1410\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.13753\n",
            "Epoch 101/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1678 - val_loss: 0.1409\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.13753\n",
            "Epoch 102/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1679 - val_loss: 0.1398\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.13753\n",
            "Epoch 103/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1723 - val_loss: 0.1415\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.13753\n",
            "Epoch 104/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1673 - val_loss: 0.1379\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.13753\n",
            "Epoch 105/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1689 - val_loss: 0.1417\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.13753\n",
            "Epoch 106/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1644 - val_loss: 0.1399\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.13753\n",
            "Epoch 107/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1718 - val_loss: 0.1461\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.13753\n",
            "Epoch 108/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1688 - val_loss: 0.1384\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.13753\n",
            "Epoch 109/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1704 - val_loss: 0.1378\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.13753\n",
            "Epoch 110/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1614 - val_loss: 0.1407\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.13753\n",
            "Epoch 111/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1654 - val_loss: 0.1400\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.13753\n",
            "Epoch 112/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1650 - val_loss: 0.1420\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.13753\n",
            "Epoch 113/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1670 - val_loss: 0.1406\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.13753\n",
            "Epoch 114/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1589 - val_loss: 0.1357\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.13753 to 0.13569, saving model to ./model_a_114_0.16183_0.13569_.hdf5\n",
            "Epoch 115/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1637 - val_loss: 0.1393\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.13569\n",
            "Epoch 116/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1617 - val_loss: 0.1390\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.13569\n",
            "Epoch 117/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1625 - val_loss: 0.1369\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.13569\n",
            "Epoch 118/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1645 - val_loss: 0.1381\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.13569\n",
            "Epoch 119/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1712 - val_loss: 0.1409\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.13569\n",
            "Epoch 120/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1684 - val_loss: 0.1424\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.13569\n",
            "Epoch 121/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1655 - val_loss: 0.1380\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.13569\n",
            "Epoch 122/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1620 - val_loss: 0.1417\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.13569\n",
            "Epoch 123/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1633 - val_loss: 0.1408\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.13569\n",
            "Epoch 124/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1709 - val_loss: 0.1399\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.13569\n",
            "Epoch 125/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1605 - val_loss: 0.1397\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.13569\n",
            "Epoch 126/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1648 - val_loss: 0.1379\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.13569\n",
            "Epoch 127/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1681 - val_loss: 0.1379\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.13569\n",
            "Epoch 128/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1611 - val_loss: 0.1412\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.13569\n",
            "Epoch 129/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1676 - val_loss: 0.1387\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.13569\n",
            "Epoch 130/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1608 - val_loss: 0.1366\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.13569\n",
            "Epoch 131/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1569 - val_loss: 0.1375\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.13569\n",
            "Epoch 132/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1667 - val_loss: 0.1385\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.13569\n",
            "Epoch 133/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1660 - val_loss: 0.1372\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.13569\n",
            "Epoch 134/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1598 - val_loss: 0.1363\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.13569\n",
            "Epoch 135/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1659 - val_loss: 0.1343\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.13569 to 0.13426, saving model to ./model_a_135_0.16527_0.13426_.hdf5\n",
            "Epoch 136/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1665 - val_loss: 0.1370\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.13426\n",
            "Epoch 137/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1617 - val_loss: 0.1421\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.13426\n",
            "Epoch 138/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1610 - val_loss: 0.1363\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.13426\n",
            "Epoch 139/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1599 - val_loss: 0.1365\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.13426\n",
            "Epoch 140/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1638 - val_loss: 0.1335\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.13426 to 0.13354, saving model to ./model_a_140_0.16113_0.13354_.hdf5\n",
            "Epoch 141/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1588 - val_loss: 0.1373\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.13354\n",
            "Epoch 142/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1686 - val_loss: 0.1367\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.13354\n",
            "Epoch 143/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1721 - val_loss: 0.1376\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.13354\n",
            "Epoch 144/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1612 - val_loss: 0.1360\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.13354\n",
            "Epoch 145/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1592 - val_loss: 0.1374\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.13354\n",
            "Epoch 146/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1623 - val_loss: 0.1380\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.13354\n",
            "Epoch 147/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1621 - val_loss: 0.1366\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.13354\n",
            "Epoch 148/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1673 - val_loss: 0.1369\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.13354\n",
            "Epoch 149/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1628 - val_loss: 0.1376\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.13354\n",
            "Epoch 150/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1586 - val_loss: 0.1388\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.13354\n",
            "Epoch 151/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1627 - val_loss: 0.1357\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.13354\n",
            "Epoch 152/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1622 - val_loss: 0.1367\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.13354\n",
            "Epoch 153/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1624 - val_loss: 0.1366\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.13354\n",
            "Epoch 154/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1625 - val_loss: 0.1317\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.13354 to 0.13165, saving model to ./model_a_154_0.16395_0.13165_.hdf5\n",
            "Epoch 155/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1556 - val_loss: 0.1351\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.13165\n",
            "Epoch 156/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1659 - val_loss: 0.1328\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.13165\n",
            "Epoch 157/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1617 - val_loss: 0.1392\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.13165\n",
            "Epoch 158/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1580 - val_loss: 0.1377\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.13165\n",
            "Epoch 159/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1604 - val_loss: 0.1355\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.13165\n",
            "Epoch 160/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1561 - val_loss: 0.1349\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.13165\n",
            "Epoch 161/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1476 - val_loss: 0.1373\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.13165\n",
            "Epoch 162/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1573 - val_loss: 0.1342\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.13165\n",
            "Epoch 163/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1610 - val_loss: 0.1342\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.13165\n",
            "Epoch 164/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1629 - val_loss: 0.1340\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.13165\n",
            "Epoch 165/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1609 - val_loss: 0.1316\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.13165 to 0.13156, saving model to ./model_a_165_0.15951_0.13156_.hdf5\n",
            "Epoch 166/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1554 - val_loss: 0.1345\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.13156\n",
            "Epoch 167/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1559 - val_loss: 0.1327\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.13156\n",
            "Epoch 168/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1587 - val_loss: 0.1322\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.13156\n",
            "Epoch 169/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1544 - val_loss: 0.1306\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.13156 to 0.13062, saving model to ./model_a_169_0.15531_0.13062_.hdf5\n",
            "Epoch 170/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1565 - val_loss: 0.1314\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.13062\n",
            "Epoch 171/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1593 - val_loss: 0.1331\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.13062\n",
            "Epoch 172/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1556 - val_loss: 0.1311\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.13062\n",
            "Epoch 173/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1587 - val_loss: 0.1330\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.13062\n",
            "Epoch 174/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1589 - val_loss: 0.1344\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.13062\n",
            "Epoch 175/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1547 - val_loss: 0.1340\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.13062\n",
            "Epoch 176/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1583 - val_loss: 0.1390\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.13062\n",
            "Epoch 177/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1616 - val_loss: 0.1353\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.13062\n",
            "Epoch 178/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1590 - val_loss: 0.1354\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.13062\n",
            "Epoch 179/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1554 - val_loss: 0.1320\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.13062\n",
            "Epoch 180/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1550 - val_loss: 0.1371\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.13062\n",
            "Epoch 181/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1582 - val_loss: 0.1318\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.13062\n",
            "Epoch 182/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1567 - val_loss: 0.1315\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.13062\n",
            "Epoch 183/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1523 - val_loss: 0.1321\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.13062\n",
            "Epoch 184/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1557 - val_loss: 0.1342\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.13062\n",
            "Epoch 185/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1602 - val_loss: 0.1304\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.13062 to 0.13041, saving model to ./model_a_185_0.15558_0.13041_.hdf5\n",
            "Epoch 186/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1632 - val_loss: 0.1326\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.13041\n",
            "Epoch 187/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1584 - val_loss: 0.1325\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.13041\n",
            "Epoch 188/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1565 - val_loss: 0.1313\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.13041\n",
            "Epoch 189/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1569 - val_loss: 0.1294\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.13041 to 0.12941, saving model to ./model_a_189_0.15362_0.12941_.hdf5\n",
            "Epoch 190/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1530 - val_loss: 0.1352\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.12941\n",
            "Epoch 191/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1535 - val_loss: 0.1315\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.12941\n",
            "Epoch 192/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1554 - val_loss: 0.1290\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.12941 to 0.12899, saving model to ./model_a_192_0.15606_0.12899_.hdf5\n",
            "Epoch 193/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1601 - val_loss: 0.1306\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.12899\n",
            "Epoch 194/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1522 - val_loss: 0.1306\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.12899\n",
            "Epoch 195/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1522 - val_loss: 0.1313\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.12899\n",
            "Epoch 196/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1518 - val_loss: 0.1339\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.12899\n",
            "Epoch 197/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1547 - val_loss: 0.1299\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.12899\n",
            "Epoch 198/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1513 - val_loss: 0.1290\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.12899\n",
            "Epoch 199/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1573 - val_loss: 0.1317\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.12899\n",
            "Epoch 200/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1606 - val_loss: 0.1312\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.12899\n",
            "Epoch 201/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1516 - val_loss: 0.1301\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.12899\n",
            "Epoch 202/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1608 - val_loss: 0.1290\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.12899\n",
            "Epoch 203/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1539 - val_loss: 0.1329\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.12899\n",
            "Epoch 204/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1502 - val_loss: 0.1302\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.12899\n",
            "Epoch 205/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1521 - val_loss: 0.1269\n",
            "\n",
            "Epoch 00205: val_loss improved from 0.12899 to 0.12692, saving model to ./model_a_205_0.15286_0.12692_.hdf5\n",
            "Epoch 206/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1543 - val_loss: 0.1269\n",
            "\n",
            "Epoch 00206: val_loss improved from 0.12692 to 0.12691, saving model to ./model_a_206_0.15180_0.12691_.hdf5\n",
            "Epoch 207/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1525 - val_loss: 0.1302\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.12691\n",
            "Epoch 208/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1517 - val_loss: 0.1391\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.12691\n",
            "Epoch 209/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1586 - val_loss: 0.1289\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.12691\n",
            "Epoch 210/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1533 - val_loss: 0.1293\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.12691\n",
            "Epoch 211/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1554 - val_loss: 0.1298\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.12691\n",
            "Epoch 212/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1503 - val_loss: 0.1288\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.12691\n",
            "Epoch 213/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1551 - val_loss: 0.1268\n",
            "\n",
            "Epoch 00213: val_loss improved from 0.12691 to 0.12684, saving model to ./model_a_213_0.15447_0.12684_.hdf5\n",
            "Epoch 214/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1516 - val_loss: 0.1277\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.12684\n",
            "Epoch 215/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1488 - val_loss: 0.1277\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.12684\n",
            "Epoch 216/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1459 - val_loss: 0.1284\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.12684\n",
            "Epoch 217/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1466 - val_loss: 0.1323\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.12684\n",
            "Epoch 218/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1575 - val_loss: 0.1298\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.12684\n",
            "Epoch 219/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1502 - val_loss: 0.1287\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.12684\n",
            "Epoch 220/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1471 - val_loss: 0.1300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.12684\n",
            "Epoch 221/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1494 - val_loss: 0.1313\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.12684\n",
            "Epoch 222/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1526 - val_loss: 0.1295\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.12684\n",
            "Epoch 223/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1513 - val_loss: 0.1275\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.12684\n",
            "Epoch 224/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1515 - val_loss: 0.1301\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.12684\n",
            "Epoch 225/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1513 - val_loss: 0.1291\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.12684\n",
            "Epoch 226/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1526 - val_loss: 0.1274\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.12684\n",
            "Epoch 227/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1572 - val_loss: 0.1294\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.12684\n",
            "Epoch 228/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1534 - val_loss: 0.1289\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.12684\n",
            "Epoch 229/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1516 - val_loss: 0.1291\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.12684\n",
            "Epoch 230/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1515 - val_loss: 0.1285\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.12684\n",
            "Epoch 231/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1545 - val_loss: 0.1269\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.12684\n",
            "Epoch 232/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1497 - val_loss: 0.1289\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.12684\n",
            "Epoch 233/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1520 - val_loss: 0.1298\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.12684\n",
            "Epoch 234/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1505 - val_loss: 0.1316\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.12684\n",
            "Epoch 235/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1493 - val_loss: 0.1262\n",
            "\n",
            "Epoch 00235: val_loss improved from 0.12684 to 0.12623, saving model to ./model_a_235_0.14932_0.12623_.hdf5\n",
            "Epoch 236/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1502 - val_loss: 0.1269\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.12623\n",
            "Epoch 237/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1476 - val_loss: 0.1263\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.12623\n",
            "Epoch 238/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1464 - val_loss: 0.1301\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.12623\n",
            "Epoch 239/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1510 - val_loss: 0.1247\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.12623 to 0.12469, saving model to ./model_a_239_0.15249_0.12469_.hdf5\n",
            "Epoch 240/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1531 - val_loss: 0.1268\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.12469\n",
            "Epoch 241/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1500 - val_loss: 0.1293\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.12469\n",
            "Epoch 242/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1516 - val_loss: 0.1297\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.12469\n",
            "Epoch 243/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1485 - val_loss: 0.1293\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.12469\n",
            "Epoch 244/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1535 - val_loss: 0.1254\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.12469\n",
            "Epoch 245/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1545 - val_loss: 0.1261\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.12469\n",
            "Epoch 246/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1546 - val_loss: 0.1307\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.12469\n",
            "Epoch 247/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1513 - val_loss: 0.1256\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.12469\n",
            "Epoch 248/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1463 - val_loss: 0.1265\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.12469\n",
            "Epoch 249/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1477 - val_loss: 0.1287\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.12469\n",
            "Epoch 250/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1459 - val_loss: 0.1291\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.12469\n",
            "Epoch 251/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1512 - val_loss: 0.1263\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.12469\n",
            "Epoch 252/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1451 - val_loss: 0.1320\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.12469\n",
            "Epoch 253/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1469 - val_loss: 0.1282\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.12469\n",
            "Epoch 254/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1494 - val_loss: 0.1316\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.12469\n",
            "Epoch 255/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1481 - val_loss: 0.1255\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.12469\n",
            "Epoch 256/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1485 - val_loss: 0.1256\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.12469\n",
            "Epoch 257/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1489 - val_loss: 0.1262\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.12469\n",
            "Epoch 258/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1497 - val_loss: 0.1278\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.12469\n",
            "Epoch 259/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1427 - val_loss: 0.1280\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.12469\n",
            "Epoch 260/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1473 - val_loss: 0.1256\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.12469\n",
            "Epoch 261/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1497 - val_loss: 0.1305\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.12469\n",
            "Epoch 262/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1509 - val_loss: 0.1253\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.12469\n",
            "Epoch 263/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1439 - val_loss: 0.1270\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.12469\n",
            "Epoch 264/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1488 - val_loss: 0.1260\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.12469\n",
            "Epoch 265/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1505 - val_loss: 0.1277\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.12469\n",
            "Epoch 266/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1485 - val_loss: 0.1271\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.12469\n",
            "Epoch 267/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1475 - val_loss: 0.1262\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.12469\n",
            "Epoch 268/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1470 - val_loss: 0.1302\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.12469\n",
            "Epoch 269/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1468 - val_loss: 0.1270\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.12469\n",
            "Epoch 270/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1516 - val_loss: 0.1264\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.12469\n",
            "Epoch 271/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1473 - val_loss: 0.1278\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.12469\n",
            "Epoch 272/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1481 - val_loss: 0.1264\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.12469\n",
            "Epoch 273/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1468 - val_loss: 0.1243\n",
            "\n",
            "Epoch 00273: val_loss improved from 0.12469 to 0.12433, saving model to ./model_a_273_0.15000_0.12433_.hdf5\n",
            "Epoch 274/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1509 - val_loss: 0.1244\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.12433\n",
            "Epoch 275/1500\n",
            "466/466 [==============================] - 1s 2ms/step - loss: 0.1456 - val_loss: 0.1296\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.12433\n",
            "Epoch 276/1500\n",
            "466/466 [==============================] - 1s 1ms/step - loss: 0.1465 - val_loss: 0.1279\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.12433\n",
            "Epoch 277/1500\n",
            "119/466 [======>.......................] - ETA: 0s - loss: 0.1410"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}